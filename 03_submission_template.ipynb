{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcfadcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import kaggle_evaluation.default_inference_server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921a2089",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4f043e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data path\n",
    "DATA_PATH = Path('/kaggle/input/hull-tactical-market-prediction/')\n",
    "\n",
    "# Signal conversion parameters (TUNE THESE FROM YOUR CV!)\n",
    "SIGNAL_MULTIPLIER_ENET = 400.0\n",
    "SIGNAL_MULTIPLIER_LGBM = 400.0\n",
    "\n",
    "# Ensemble weights (TUNE THESE FROM YOUR CV!)\n",
    "WEIGHT_ENET = 0.3\n",
    "WEIGHT_LGBM = 0.7\n",
    "\n",
    "# Date cutoff (remove early sparse data)\n",
    "CUTOFF_DATE = 1000\n",
    "\n",
    "# Global variables\n",
    "MODEL_ENET = None\n",
    "MODEL_LGBM = None\n",
    "FEATURE_COLS = None\n",
    "FITTED = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ae6ea0",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a4e2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_return_to_signal(predicted_return, multiplier):\n",
    "    \"\"\"\n",
    "    Map predicted return → allocation [0, 2]\n",
    "    \"\"\"\n",
    "    signal = predicted_return * multiplier + 1.0\n",
    "    return np.clip(signal, 0.0, 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7301c8",
   "metadata": {},
   "source": [
    "## Model Training (Run Once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39dafde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models():\n",
    "    \"\"\"\n",
    "    Train both ElasticNet and LightGBM on full training data.\n",
    "    Called once on first predict() call.\n",
    "    \"\"\"\n",
    "    global MODEL_ENET, MODEL_LGBM, FEATURE_COLS, FITTED\n",
    "    \n",
    "    print(\"Loading training data...\")\n",
    "    train_df = pd.read_csv(DATA_PATH / 'train.csv')\n",
    "    \n",
    "    # Trim early sparse dates\n",
    "    train_df = train_df[train_df['date_id'] >= CUTOFF_DATE].reset_index(drop=True)\n",
    "    print(f\"Training on {len(train_df)} samples (date_id >= {CUTOFF_DATE})\")\n",
    "    \n",
    "    # Define features\n",
    "    FEATURE_COLS = [c for c in train_df.columns \n",
    "                    if c.startswith(('M', 'E', 'I', 'P', 'V', 'S', 'MOM', 'D'))]\n",
    "    \n",
    "    X = train_df[FEATURE_COLS]\n",
    "    y = train_df['market_forward_excess_returns']\n",
    "    \n",
    "    # Drop rows with NaN target\n",
    "    mask = y.notna()\n",
    "    X = X[mask]\n",
    "    y = y[mask]\n",
    "    \n",
    "    print(f\"Training samples after dropping NaN targets: {len(X)}\")\n",
    "    \n",
    "    # ==================== Train ElasticNet ====================\n",
    "    print(\"\\nTraining ElasticNet...\")\n",
    "    MODEL_ENET = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', ElasticNet(\n",
    "            alpha=0.1,  # Use best alpha from your CV!\n",
    "            l1_ratio=0.5,\n",
    "            max_iter=100000\n",
    "        ))\n",
    "    ])\n",
    "    MODEL_ENET.fit(X, y)\n",
    "    print(\"ElasticNet trained.\")\n",
    "    \n",
    "    # ==================== Train LightGBM ====================\n",
    "    print(\"\\nTraining LightGBM...\")\n",
    "    X_lgbm = X.fillna(-999)\n",
    "    train_set = lgb.Dataset(X_lgbm, label=y)\n",
    "    \n",
    "    lgb_params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'learning_rate': 0.01,\n",
    "        'num_leaves': 63,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    MODEL_LGBM = lgb.train(\n",
    "        lgb_params,\n",
    "        train_set,\n",
    "        num_boost_round=3000,  # Use best n_round from your CV!\n",
    "        valid_sets=[train_set],\n",
    "        callbacks=[lgb.log_evaluation(0)]\n",
    "    )\n",
    "    print(\"LightGBM trained.\")\n",
    "    \n",
    "    FITTED = True\n",
    "    print(\"\\n✅ All models trained and ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92312627",
   "metadata": {},
   "source": [
    "## Prediction Function (Kaggle API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0a855f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test: pl.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Main prediction function called by Kaggle evaluation API.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    test : pl.DataFrame\n",
    "        Polars DataFrame with features for one or more test samples\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float or np.ndarray : Allocation(s) between 0.0 and 2.0\n",
    "    \"\"\"\n",
    "    global FITTED, MODEL_ENET, MODEL_LGBM, FEATURE_COLS\n",
    "    \n",
    "    # Train models on first call\n",
    "    if not FITTED:\n",
    "        train_models()\n",
    "    \n",
    "    # Convert to pandas\n",
    "    test_pd = test.to_pandas()\n",
    "    \n",
    "    # Extract features\n",
    "    X_test = test_pd[FEATURE_COLS]\n",
    "    \n",
    "    # ==================== ElasticNet Prediction ====================\n",
    "    pred_returns_enet = MODEL_ENET.predict(X_test)\n",
    "    signal_enet = convert_return_to_signal(pred_returns_enet, SIGNAL_MULTIPLIER_ENET)\n",
    "    \n",
    "    # ==================== LightGBM Prediction ====================\n",
    "    X_test_lgbm = X_test.fillna(-999)\n",
    "    pred_returns_lgbm = MODEL_LGBM.predict(X_test_lgbm)\n",
    "    signal_lgbm = convert_return_to_signal(pred_returns_lgbm, SIGNAL_MULTIPLIER_LGBM)\n",
    "    \n",
    "    # ==================== Ensemble ====================\n",
    "    final_signal = (\n",
    "        WEIGHT_ENET * signal_enet +\n",
    "        WEIGHT_LGBM * signal_lgbm\n",
    "    )\n",
    "    \n",
    "    final_signal = np.clip(final_signal, 0.0, 2.0)\n",
    "    \n",
    "    # Return scalar if single sample, else array\n",
    "    if len(final_signal) == 1:\n",
    "        return float(final_signal[0])\n",
    "    else:\n",
    "        return final_signal.astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f5d9d9",
   "metadata": {},
   "source": [
    "## Launch Inference Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9d4752",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    # Production mode: serve predictions to Kaggle\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    # Local testing mode\n",
    "    inference_server.run_local_gateway((str(DATA_PATH),))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83325653",
   "metadata": {},
   "source": [
    "---\n",
    "## ✅ Submission Checklist:\n",
    "1. ✅ No leakage (no `true_targets` lookup)\n",
    "2. ✅ Models trained only on past data\n",
    "3. ✅ Proper signal conversion\n",
    "4. ✅ Ensemble weights from CV\n",
    "5. ✅ Kaggle API integration\n",
    "\n",
    "**Before submitting:**\n",
    "- Update `SIGNAL_MULTIPLIER_*` from CV results\n",
    "- Update `WEIGHT_*` from CV results\n",
    "- Test locally first!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
