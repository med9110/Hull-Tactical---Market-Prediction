{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc1352ab",
   "metadata": {
    "papermill": {
     "duration": 0.007303,
     "end_time": "2025-10-28T03:39:14.408137",
     "exception": false,
     "start_time": "2025-10-28T03:39:14.400834",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### public solutions:\n",
    "\n",
    "- [Model 7](#Model_7) - Lb=[17.396](https://www.kaggle.com/code/baidalinadilzhan/hull-tactical-lb-17-396?scriptVersionId=262804590) - v.1 - [hull-tactical-lb-17.396](https://www.kaggle.com/code/baidalinadilzhan/hull-tactical-lb-17-396)\n",
    "- [Model 6](#Model_6) - Lb=[10.237](https://www.kaggle.com/code/veniaminnelin/hull-tactical-leaderboard-lol?scriptVersionId=262531157) - v.3 - [Hull Tactical - Leaderboard LOL](https://www.kaggle.com/code/veniaminnelin/hull-tactical-leaderboard-lol).2\n",
    "- [Model 5](#Model_5) - Lb=[10.217](https://www.kaggle.com/code/mbrosseau/hull-tactical-max-leaderboard?scriptVersionId=262493413) - v.9 - [Hull Tactical - Max Leaderboard](https://www.kaggle.com/code/mbrosseau/hull-tactical-max-leaderboard)\n",
    "- [Model 4](#Model_4) - Lb=[10.164](https://www.kaggle.com/code/mbrosseau/hull-tactical-max-leaderboard?scriptVersionId=262493413) - v.4 - [Hull Tactical - Max Leaderboard](https://www.kaggle.com/code/mbrosseau/hull-tactical-max-leaderboard)\n",
    "- [Model 1](#Model_1) - Lb=[10.147](https://www.kaggle.com/code/veniaminnelin/hull-tactical-leaderboard-lol?scriptVersionId=262460746) - v.1 - [Hull Tactical - Leaderboard LOL](https://www.kaggle.com/code/veniaminnelin/hull-tactical-leaderboard-lol).1\n",
    "- [Model 2](#Model_2) - LB=[10.005](https://www.kaggle.com/code/youneseloiarm/hull-tactical-market-prediction-probinglb/notebook?scriptVersionId=262450829) - v.4 - [Hull Tactical - Market Prediction - ProbingLB\n",
    "](https://www.kaggle.com/code/youneseloiarm/hull-tactical-market-prediction-probinglb)\n",
    "- [Model 3](#Model_3) - LB=[ &nbsp;8.093](https://www.kaggle.com/code/imaadmahmood/hull-market-prediction?scriptVersionId=262297550) - v.4 - [Hull Market Prediction\n",
    "](https://www.kaggle.com/code/imaadmahmood/hull-market-predictionb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f56c9f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T03:39:14.423467Z",
     "iopub.status.busy": "2025-10-28T03:39:14.423127Z",
     "iopub.status.idle": "2025-10-28T03:39:17.931393Z",
     "shell.execute_reply": "2025-10-28T03:39:17.929907Z"
    },
    "papermill": {
     "duration": 3.518198,
     "end_time": "2025-10-28T03:39:17.933455",
     "exception": false,
     "start_time": "2025-10-28T03:39:14.415257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kaggle_evaluation.default_inference_server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7555c7",
   "metadata": {
    "papermill": {
     "duration": 0.006172,
     "end_time": "2025-10-28T03:39:17.946935",
     "exception": false,
     "start_time": "2025-10-28T03:39:17.940763",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a9a158",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "papermill": {
     "duration": 0.00572,
     "end_time": "2025-10-28T03:39:17.958777",
     "exception": false,
     "start_time": "2025-10-28T03:39:17.953057",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Since in this competition the leaderboard does not really matter, as all test data is included in the training set, I was simply curious to see what the maximum possible score of the metric could be if we had perfect knowledge of the \"future\" market behavior, and to better understand how the evaluation metric works.\n",
    "\n",
    "(And it was also fun to get to the first position on the leaderboard at least once in my life, even if only for a short while =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50a43a91",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-10-28T03:39:17.972465Z",
     "iopub.status.busy": "2025-10-28T03:39:17.972010Z",
     "iopub.status.idle": "2025-10-28T03:39:17.977284Z",
     "shell.execute_reply": "2025-10-28T03:39:17.976328Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.014174,
     "end_time": "2025-10-28T03:39:17.978999",
     "exception": false,
     "start_time": "2025-10-28T03:39:17.964825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e35aced",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-10-28T03:39:17.993670Z",
     "iopub.status.busy": "2025-10-28T03:39:17.993280Z",
     "iopub.status.idle": "2025-10-28T03:39:18.301721Z",
     "shell.execute_reply": "2025-10-28T03:39:18.300580Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.317335,
     "end_time": "2025-10-28T03:39:18.303461",
     "exception": false,
     "start_time": "2025-10-28T03:39:17.986126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_INVESTMENT = 2\n",
    "MIN_INVESTMENT = 0\n",
    "DATA_PATH: Path = Path('/kaggle/input/hull-tactical-market-prediction/')\n",
    "\n",
    "_true_train_df = pl.read_csv(DATA_PATH / \"train.csv\").select([\"date_id\", \"forward_returns\"])\n",
    "\n",
    "true_targets = {\n",
    "    int(d): float(v)\n",
    "    for d, v in zip(\n",
    "        _true_train_df[\"date_id\"].to_numpy(),\n",
    "        _true_train_df[\"forward_returns\"].to_numpy()\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc4ea3b1",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-10-28T03:39:18.317529Z",
     "iopub.status.busy": "2025-10-28T03:39:18.317156Z",
     "iopub.status.idle": "2025-10-28T03:39:18.323903Z",
     "shell.execute_reply": "2025-10-28T03:39:18.322763Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.016679,
     "end_time": "2025-10-28T03:39:18.326505",
     "exception": false,
     "start_time": "2025-10-28T03:39:18.309826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_Model_1(test: pl.DataFrame) -> float:\n",
    "    date_id = int(test.select(\"date_id\").to_series().item())\n",
    "    t = true_targets.get(date_id, None)  \n",
    "    pred = MAX_INVESTMENT if t > 0 else MIN_INVESTMENT\n",
    "    print(f'{pred}')\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1336ed24",
   "metadata": {
    "papermill": {
     "duration": 0.006115,
     "end_time": "2025-10-28T03:39:18.339400",
     "exception": false,
     "start_time": "2025-10-28T03:39:18.333285",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6652815a",
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "execution": {
     "iopub.execute_input": "2025-10-28T03:39:18.353984Z",
     "iopub.status.busy": "2025-10-28T03:39:18.353618Z",
     "iopub.status.idle": "2025-10-28T03:40:12.340262Z",
     "shell.execute_reply": "2025-10-28T03:40:12.339053Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 54.004591,
     "end_time": "2025-10-28T03:40:12.350389",
     "exception": false,
     "start_time": "2025-10-28T03:39:18.345798",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13/3349983318.py:40: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  for i in zip(data.columns, data.dtypes): data[i[0]].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_13/3349983318.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  for i in zip(data.columns, data.dtypes): data[i[0]].fillna(0, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StackingRegressor(cv=3,\n",
       "                  estimators=[(&#x27;CatBoost&#x27;,\n",
       "                               &lt;catboost.core.CatBoostRegressor object at 0x787cba91b550&gt;),\n",
       "                              (&#x27;XGBoost&#x27;,\n",
       "                               XGBRegressor(base_score=None, booster=None,\n",
       "                                            callbacks=None,\n",
       "                                            colsample_bylevel=None,\n",
       "                                            colsample_bynode=None,\n",
       "                                            colsample_bytree=0.7, device=None,\n",
       "                                            early_stopping_rounds=None,\n",
       "                                            enable_categorical=False,\n",
       "                                            eval_metric=None,\n",
       "                                            feature_types=None, gamma=None,\n",
       "                                            g...\n",
       "                                                     min_samples_split=5,\n",
       "                                                     random_state=42)),\n",
       "                              (&#x27;ExtraTrees&#x27;,\n",
       "                               ExtraTreesRegressor(max_depth=12,\n",
       "                                                   max_features=&#x27;sqrt&#x27;,\n",
       "                                                   min_samples_leaf=3,\n",
       "                                                   min_samples_split=5,\n",
       "                                                   random_state=42)),\n",
       "                              (&#x27;GBRegressor&#x27;,\n",
       "                               GradientBoostingRegressor(max_depth=8,\n",
       "                                                         max_features=&#x27;sqrt&#x27;,\n",
       "                                                         min_samples_leaf=50,\n",
       "                                                         min_samples_split=500,\n",
       "                                                         random_state=10,\n",
       "                                                         subsample=0.8))],\n",
       "                  final_estimator=RidgeCV(alphas=[0.1, 1.0, 10.0, 100.0]))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StackingRegressor</label><div class=\"sk-toggleable__content\"><pre>StackingRegressor(cv=3,\n",
       "                  estimators=[(&#x27;CatBoost&#x27;,\n",
       "                               &lt;catboost.core.CatBoostRegressor object at 0x787cba91b550&gt;),\n",
       "                              (&#x27;XGBoost&#x27;,\n",
       "                               XGBRegressor(base_score=None, booster=None,\n",
       "                                            callbacks=None,\n",
       "                                            colsample_bylevel=None,\n",
       "                                            colsample_bynode=None,\n",
       "                                            colsample_bytree=0.7, device=None,\n",
       "                                            early_stopping_rounds=None,\n",
       "                                            enable_categorical=False,\n",
       "                                            eval_metric=None,\n",
       "                                            feature_types=None, gamma=None,\n",
       "                                            g...\n",
       "                                                     min_samples_split=5,\n",
       "                                                     random_state=42)),\n",
       "                              (&#x27;ExtraTrees&#x27;,\n",
       "                               ExtraTreesRegressor(max_depth=12,\n",
       "                                                   max_features=&#x27;sqrt&#x27;,\n",
       "                                                   min_samples_leaf=3,\n",
       "                                                   min_samples_split=5,\n",
       "                                                   random_state=42)),\n",
       "                              (&#x27;GBRegressor&#x27;,\n",
       "                               GradientBoostingRegressor(max_depth=8,\n",
       "                                                         max_features=&#x27;sqrt&#x27;,\n",
       "                                                         min_samples_leaf=50,\n",
       "                                                         min_samples_split=500,\n",
       "                                                         random_state=10,\n",
       "                                                         subsample=0.8))],\n",
       "                  final_estimator=RidgeCV(alphas=[0.1, 1.0, 10.0, 100.0]))</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>CatBoost</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CatBoostRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;catboost.core.CatBoostRegressor object at 0x787cba91b550&gt;</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>XGBoost</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=0.7, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=6, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=1500, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=42, ...)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>LGBM</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMRegressor</label><div class=\"sk-toggleable__content\"><pre>LGBMRegressor(learning_rate=0.05, max_depth=8, n_estimators=1500, num_leaves=50,\n",
       "              random_state=42, reg_alpha=1.0, reg_lambda=1.0, verbosity=-1)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>RandomForest</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=15, max_features=&#x27;sqrt&#x27;, min_samples_leaf=3,\n",
       "                      min_samples_split=5, random_state=42)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>ExtraTrees</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ExtraTreesRegressor</label><div class=\"sk-toggleable__content\"><pre>ExtraTreesRegressor(max_depth=12, max_features=&#x27;sqrt&#x27;, min_samples_leaf=3,\n",
       "                    min_samples_split=5, random_state=42)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>GBRegressor</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(max_depth=8, max_features=&#x27;sqrt&#x27;, min_samples_leaf=50,\n",
       "                          min_samples_split=500, random_state=10,\n",
       "                          subsample=0.8)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>final_estimator</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RidgeCV</label><div class=\"sk-toggleable__content\"><pre>RidgeCV(alphas=[0.1, 1.0, 10.0, 100.0])</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "StackingRegressor(cv=3,\n",
       "                  estimators=[('CatBoost',\n",
       "                               <catboost.core.CatBoostRegressor object at 0x787cba91b550>),\n",
       "                              ('XGBoost',\n",
       "                               XGBRegressor(base_score=None, booster=None,\n",
       "                                            callbacks=None,\n",
       "                                            colsample_bylevel=None,\n",
       "                                            colsample_bynode=None,\n",
       "                                            colsample_bytree=0.7, device=None,\n",
       "                                            early_stopping_rounds=None,\n",
       "                                            enable_categorical=False,\n",
       "                                            eval_metric=None,\n",
       "                                            feature_types=None, gamma=None,\n",
       "                                            g...\n",
       "                                                     min_samples_split=5,\n",
       "                                                     random_state=42)),\n",
       "                              ('ExtraTrees',\n",
       "                               ExtraTreesRegressor(max_depth=12,\n",
       "                                                   max_features='sqrt',\n",
       "                                                   min_samples_leaf=3,\n",
       "                                                   min_samples_split=5,\n",
       "                                                   random_state=42)),\n",
       "                              ('GBRegressor',\n",
       "                               GradientBoostingRegressor(max_depth=8,\n",
       "                                                         max_features='sqrt',\n",
       "                                                         min_samples_leaf=50,\n",
       "                                                         min_samples_split=500,\n",
       "                                                         random_state=10,\n",
       "                                                         subsample=0.8))],\n",
       "                  final_estimator=RidgeCV(alphas=[0.1, 1.0, 10.0, 100.0]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "import pandas as pd, polars as pl, numpy as np\n",
    "\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from xgboost  import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor,Pool\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from sklearn.preprocessing   import StandardScaler\n",
    "# from sklearn.model_selection import KFold, cross_val_score, train_test_split \n",
    "\n",
    "train = pd.read_csv('/kaggle/input/hull-tactical-market-prediction/train.csv').dropna()\n",
    "test  = pd.read_csv('/kaggle/input/hull-tactical-market-prediction/test.csv').dropna()\n",
    "\n",
    "\n",
    "def preprocessing(data, typ):\n",
    "    main_feature = [\n",
    "        'E1',  'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8', 'E9','E10', \n",
    "        'E11','E12','E13','E14','E15','E16','E17','E18','E19','E20', \n",
    "               \"I2\",            \n",
    "                                                   \"P8\", \"P9\",\"P10\", \n",
    "              \"P12\",\"P13\",\n",
    "        \"S1\",  \"S2\",             \"S5\"\n",
    "    ]\n",
    "    \n",
    "    if typ == \"train\":\n",
    "        data = data[main_feature + [\"forward_returns\"]]\n",
    "    else:\n",
    "        data = data[main_feature]\n",
    "        \n",
    "    for i in zip(data.columns, data.dtypes): data[i[0]].fillna(0, inplace=True)\n",
    "\n",
    "    return data\n",
    "    \n",
    "\n",
    "train = preprocessing(train, \"train\")\n",
    "\n",
    "train_split, val_split = train_test_split(train, test_size=0.01, random_state=4)\n",
    "\n",
    "X_train = train_split.drop(columns=[\"forward_returns\"])\n",
    "X_test  = val_split  .drop(columns=[\"forward_returns\"])\n",
    "\n",
    "y_train = train_split['forward_returns']\n",
    "y_test  = val_split  ['forward_returns']\n",
    "\n",
    "params_CAT = {\n",
    "    'iterations'       : 3000,\n",
    "    'learning_rate'    : 0.01,\n",
    "    'depth'            : 6,\n",
    "    'l2_leaf_reg'      : 5.0,\n",
    "    'min_child_samples': 100,\n",
    "    'colsample_bylevel': 0.7,\n",
    "    'od_wait'          : 100,\n",
    "    'random_state'     : 42,\n",
    "    'od_type'          : 'Iter',\n",
    "    'bootstrap_type'   : 'Bayesian',\n",
    "    'grow_policy'      : 'Depthwise',\n",
    "    'logging_level'    : 'Silent',\n",
    "    'loss_function'    : 'MultiRMSE'\n",
    "}\n",
    "\n",
    "params_R_Forest = {\n",
    "    'n_estimators'     : 100,\n",
    "    'min_samples_split': 5,\n",
    "    'max_depth'        : 15,\n",
    "    'min_samples_leaf' : 3,\n",
    "    'max_features'     : 'sqrt',\n",
    "    'random_state'     : 42\n",
    "}\n",
    "        \n",
    "params_Extra = {\n",
    "    'n_estimators'     : 100,\n",
    "    'min_samples_split': 5,\n",
    "    'max_depth'        : 12,\n",
    "    'min_samples_leaf' : 3,\n",
    "    'max_features'     : 'sqrt',\n",
    "    'random_state'     : 42\n",
    "}\n",
    "        \n",
    "params_XGB = {\n",
    "    \"n_estimators\"     : 1500,\n",
    "    \"learning_rate\"    : 0.05, \n",
    "    \"max_depth\"        : 6,\n",
    "    \"subsample\"        : 0.8, \n",
    "    \"colsample_bytree\" : 0.7,\n",
    "    \"reg_alpha\"        : 1.0,\n",
    "    \"reg_lambda\"       : 1.0,\n",
    "    \"random_state\"     : 42\n",
    "}\n",
    "\n",
    "params_LGBM = {\n",
    "    \"n_estimators\"     : 1500,\n",
    "    \"learning_rate\"    : 0.05,\n",
    "    \"num_leaves\"       : 50,\n",
    "    \"max_depth\"        : 8,\n",
    "    \"reg_alpha\"        : 1.0,\n",
    "    \"reg_lambda\"       : 1.0,\n",
    "    \"random_state\"     : 42,\n",
    "    'verbosity'        : -1\n",
    "}\n",
    "\n",
    "params_DecisionTree = {\n",
    "    'criterion'        : 'poisson',     \n",
    "    'max_depth'        : 6\n",
    "}\n",
    "\n",
    "params_GB = {\n",
    "    \"learning_rate\"    : 0.1,\n",
    "    \"min_samples_split\": 500,\n",
    "    \"min_samples_leaf\" : 50,\n",
    "    \"max_depth\"        : 8,\n",
    "    \"max_features\"     : 'sqrt',\n",
    "    \"subsample\"        : 0.8,\n",
    "    \"random_state\"     : 10\n",
    "}\n",
    "\n",
    "CatBoost     = CatBoostRegressor         (**params_CAT)\n",
    "XGBoost      = XGBRegressor              (**params_XGB)\n",
    "LGBM         = LGBMRegressor             (**params_LGBM)\n",
    "RandomForest = RandomForestRegressor     (**params_R_Forest)\n",
    "ExtraTrees   = ExtraTreesRegressor       (**params_Extra)\n",
    "GBRegressor  = GradientBoostingRegressor (**params_GB)\n",
    "\n",
    "estimators = [\n",
    "    ('CatBoost',     CatBoost     ), \n",
    "    ('XGBoost',      XGBoost      ), \n",
    "    ('LGBM',         LGBM         ), \n",
    "    ('RandomForest', RandomForest ),\n",
    "    ('ExtraTrees',   ExtraTrees   ), \n",
    "    ('GBRegressor',  GBRegressor  )\n",
    "]\n",
    "\n",
    "model_3 = StackingRegressor(\n",
    "    estimators, \n",
    "    final_estimator = RidgeCV(alphas=[0.1, 1.0, 10.0, 100.0]), cv=3\n",
    ")\n",
    "\n",
    "model_3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "229a458b",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-10-28T03:40:12.366351Z",
     "iopub.status.busy": "2025-10-28T03:40:12.365948Z",
     "iopub.status.idle": "2025-10-28T03:40:12.372372Z",
     "shell.execute_reply": "2025-10-28T03:40:12.371408Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.016878,
     "end_time": "2025-10-28T03:40:12.374828",
     "exception": false,
     "start_time": "2025-10-28T03:40:12.357950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_Model_3(test: pl.DataFrame) -> float:\n",
    "    test = test.to_pandas().drop(columns=[\"lagged_forward_returns\", \"date_id\", \"is_scored\"])\n",
    "    test = preprocessing(test, \"test\")\n",
    "    raw_pred = model_3.predict(test)[0]\n",
    "    return raw_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9d4554",
   "metadata": {
    "papermill": {
     "duration": 0.00652,
     "end_time": "2025-10-28T03:40:12.389502",
     "exception": false,
     "start_time": "2025-10-28T03:40:12.382982",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fcae440",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-10-28T03:40:12.404699Z",
     "iopub.status.busy": "2025-10-28T03:40:12.404331Z",
     "iopub.status.idle": "2025-10-28T03:40:12.409719Z",
     "shell.execute_reply": "2025-10-28T03:40:12.408893Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.015013,
     "end_time": "2025-10-28T03:40:12.411566",
     "exception": false,
     "start_time": "2025-10-28T03:40:12.396553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass, asdict\n",
    "import polars as pl \n",
    "import numpy as np\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV, LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e583c1b7",
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "execution": {
     "iopub.execute_input": "2025-10-28T03:40:12.426618Z",
     "iopub.status.busy": "2025-10-28T03:40:12.426246Z",
     "iopub.status.idle": "2025-10-28T03:40:12.520879Z",
     "shell.execute_reply": "2025-10-28T03:40:12.519855Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.10451,
     "end_time": "2025-10-28T03:40:12.522981",
     "exception": false,
     "start_time": "2025-10-28T03:40:12.418471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (8_990, 98)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date_id</th><th>D1</th><th>D2</th><th>D3</th><th>D4</th><th>D5</th><th>D6</th><th>D7</th><th>D8</th><th>D9</th><th>E1</th><th>E10</th><th>E11</th><th>E12</th><th>E13</th><th>E14</th><th>E15</th><th>E16</th><th>E17</th><th>E18</th><th>E19</th><th>E2</th><th>E20</th><th>E3</th><th>E4</th><th>E5</th><th>E6</th><th>E7</th><th>E8</th><th>E9</th><th>I1</th><th>I2</th><th>I3</th><th>I4</th><th>I5</th><th>I6</th><th>I7</th><th>&hellip;</th><th>P13</th><th>P2</th><th>P3</th><th>P4</th><th>P5</th><th>P6</th><th>P7</th><th>P8</th><th>P9</th><th>S1</th><th>S10</th><th>S11</th><th>S12</th><th>S2</th><th>S3</th><th>S4</th><th>S5</th><th>S6</th><th>S7</th><th>S8</th><th>S9</th><th>V1</th><th>V10</th><th>V11</th><th>V12</th><th>V13</th><th>V2</th><th>V3</th><th>V4</th><th>V5</th><th>V6</th><th>V7</th><th>V8</th><th>V9</th><th>forward_returns</th><th>risk_free_rate</th><th>market_forward_excess_returns</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>&hellip;</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>-0.002421</td><td>0.000301</td><td>-0.003038</td></tr><tr><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>-0.008495</td><td>0.000303</td><td>-0.009114</td></tr><tr><td>2</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>-0.009624</td><td>0.000301</td><td>-0.010243</td></tr><tr><td>3</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.004662</td><td>0.000299</td><td>0.004046</td></tr><tr><td>4</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>-0.011686</td><td>0.000299</td><td>-0.012301</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>8985</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>&quot;1.56537850833686&quot;</td><td>&quot;0.18452380952381&quot;</td><td>&quot;0.0191798941798942&quot;</td><td>&quot;0.0191798941798942&quot;</td><td>&quot;0.00595238095238095&quot;</td><td>&quot;0.00595238095238095&quot;</td><td>&quot;0.911375661375661&quot;</td><td>&quot;-0.0834957603011584&quot;</td><td>&quot;-0.572446603148107&quot;</td><td>&quot;0.22363818831577&quot;</td><td>&quot;-0.122313603525027&quot;</td><td>&quot;1.20925014422219&quot;</td><td>&quot;1.54011631243565&quot;</td><td>&quot;1.6551743632761&quot;</td><td>&quot;0.0314153439153439&quot;</td><td>&quot;0.331679894179894&quot;</td><td>&quot;0.0347222222222222&quot;</td><td>&quot;0.0382692660297448&quot;</td><td>&quot;-0.301875981317616&quot;</td><td>&quot;0.91468253968254&quot;</td><td>&quot;0.274140211640212&quot;</td><td>&quot;0.984115038349353&quot;</td><td>&quot;0.0806878306878307&quot;</td><td>&quot;0.476521164021164&quot;</td><td>&quot;0.597442456305455&quot;</td><td>&quot;0.718253968253968&quot;</td><td>&quot;0.238756613756614&quot;</td><td>&hellip;</td><td>&quot;0.625330687830688&quot;</td><td>&quot;-1.35449847084931&quot;</td><td>&quot;0.0462962962962963&quot;</td><td>&quot;0.514550264550265&quot;</td><td>&quot;0.276768769975892&quot;</td><td>&quot;-0.261325600057626&quot;</td><td>&quot;0.811753887240293&quot;</td><td>&quot;1.78492936574625&quot;</td><td>&quot;0.0396825396825397&quot;</td><td>&quot;0.249933173088953&quot;</td><td>&quot;0.273148148148148&quot;</td><td>&quot;0.134920634920635&quot;</td><td>&quot;0.634464741288425&quot;</td><td>&quot;-0.446681908068479&quot;</td><td>&quot;-0.0526855763278288&quot;</td><td>&quot;0.083994708994709&quot;</td><td>&quot;0.055281954303961&quot;</td><td>&quot;0.209656084656085&quot;</td><td>&quot;0.409391534391534&quot;</td><td>&quot;0.574660952369144&quot;</td><td>&quot;0.748677248677249&quot;</td><td>&quot;0.498677248677249&quot;</td><td>&quot;-0.616394800009832&quot;</td><td>&quot;0.561838624338624&quot;</td><td>&quot;0.533730158730159&quot;</td><td>&quot;-0.432282453978163&quot;</td><td>&quot;0.78505291005291&quot;</td><td>&quot;0.46957671957672&quot;</td><td>&quot;0.837962962962963&quot;</td><td>&quot;1.22677167174681&quot;</td><td>&quot;0.822751322751323&quot;</td><td>&quot;-0.707360636419722&quot;</td><td>&quot;0.142857142857143&quot;</td><td>&quot;-0.649616421794573&quot;</td><td>0.002457</td><td>0.000155</td><td>0.00199</td></tr><tr><td>8986</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>&quot;1.56294570736285&quot;</td><td>&quot;0.184193121693122&quot;</td><td>&quot;0.0188492063492063&quot;</td><td>&quot;0.0188492063492063&quot;</td><td>&quot;0.00562169312169312&quot;</td><td>&quot;0.00562169312169312&quot;</td><td>&quot;0.911706349206349&quot;</td><td>&quot;-0.0835423385235207&quot;</td><td>&quot;-0.572080270433783&quot;</td><td>&quot;0.222909704235328&quot;</td><td>&quot;-0.732396949860551&quot;</td><td>&quot;1.22545909429746&quot;</td><td>&quot;1.53776136440687&quot;</td><td>&quot;1.67226210824883&quot;</td><td>&quot;0.0310846560846561&quot;</td><td>&quot;0.331349206349206&quot;</td><td>&quot;0.0343915343915344&quot;</td><td>&quot;0.0382047677964104&quot;</td><td>&quot;-0.301897014182979&quot;</td><td>&quot;0.915013227513228&quot;</td><td>&quot;0.26984126984127&quot;</td><td>&quot;0.904452962662242&quot;</td><td>&quot;0.0734126984126984&quot;</td><td>&quot;0.479166666666667&quot;</td><td>&quot;0.605078681561105&quot;</td><td>&quot;0.718253968253968&quot;</td><td>&quot;0.220899470899471&quot;</td><td>&hellip;</td><td>&quot;0.739417989417989&quot;</td><td>&quot;-1.38478508824334&quot;</td><td>&quot;0.232142857142857&quot;</td><td>&quot;0.379298941798942&quot;</td><td>&quot;1.19925967661965&quot;</td><td>&quot;-0.344273917030356&quot;</td><td>&quot;0.690323485609809&quot;</td><td>&quot;1.79159569953088&quot;</td><td>&quot;0.037037037037037&quot;</td><td>&quot;0.298532581791712&quot;</td><td>&quot;0.933201058201058&quot;</td><td>&quot;0.721560846560847&quot;</td><td>&quot;1.21134501564263&quot;</td><td>&quot;-0.118050110853058&quot;</td><td>&quot;-0.249315284179041&quot;</td><td>&quot;0.566798941798942&quot;</td><td>&quot;0.107330113648913&quot;</td><td>&quot;0.228174603174603&quot;</td><td>&quot;0.409391534391534&quot;</td><td>&quot;0.580932017246943&quot;</td><td>&quot;0.37037037037037&quot;</td><td>&quot;0.528439153439153&quot;</td><td>&quot;-0.642039876784022&quot;</td><td>&quot;0.587632275132275&quot;</td><td>&quot;0.526455026455027&quot;</td><td>&quot;-0.429505792996239&quot;</td><td>&quot;0.767857142857143&quot;</td><td>&quot;0.671957671957672&quot;</td><td>&quot;0.837962962962963&quot;</td><td>&quot;0.785876680219954&quot;</td><td>&quot;0.805555555555556&quot;</td><td>&quot;-0.715692186942146&quot;</td><td>&quot;0.196097883597884&quot;</td><td>&quot;-0.668289260803376&quot;</td><td>0.002312</td><td>0.000156</td><td>0.001845</td></tr><tr><td>8987</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>&quot;1.5605200537219&quot;</td><td>&quot;0.183862433862434&quot;</td><td>&quot;0.0185185185185185&quot;</td><td>&quot;0.0185185185185185&quot;</td><td>&quot;0.00529100529100529&quot;</td><td>&quot;0.00529100529100529&quot;</td><td>&quot;0.912037037037037&quot;</td><td>&quot;-0.0838740749780047&quot;</td><td>&quot;-0.572016205010642&quot;</td><td>&quot;0.222211305740983&quot;</td><td>&quot;-0.800464708548532&quot;</td><td>&quot;1.24727312781993&quot;</td><td>&quot;1.53474183594455&quot;</td><td>&quot;1.69546884570455&quot;</td><td>&quot;0.0307539682539683&quot;</td><td>&quot;0.331018518518519&quot;</td><td>&quot;0.0340608465608466&quot;</td><td>&quot;0.038118211270212&quot;</td><td>&quot;-0.301918047426536&quot;</td><td>&quot;0.915343915343915&quot;</td><td>&quot;0.273148148148148&quot;</td><td>&quot;0.842294693008398&quot;</td><td>&quot;0.0740740740740741&quot;</td><td>&quot;0.478835978835979&quot;</td><td>&quot;0.611319101072675&quot;</td><td>&quot;0.724867724867725&quot;</td><td>&quot;0.223544973544974&quot;</td><td>&hellip;</td><td>&quot;0.809193121693122&quot;</td><td>&quot;-1.42000677294771&quot;</td><td>&quot;0.849867724867725&quot;</td><td>&quot;0.375661375661376&quot;</td><td>&quot;0.429471475390351&quot;</td><td>&quot;-0.233373569970158&quot;</td><td>&quot;-0.289766464491947&quot;</td><td>&quot;1.79281602958001&quot;</td><td>&quot;0.041005291005291&quot;</td><td>&quot;0.371361679434561&quot;</td><td>&quot;0.793650793650794&quot;</td><td>&quot;0.689814814814815&quot;</td><td>&quot;0.885178015455313&quot;</td><td>&quot;-0.316882407040784&quot;</td><td>&quot;-0.422374286909508&quot;</td><td>&quot;0.631613756613757&quot;</td><td>&quot;-0.0297695460919471&quot;</td><td>&quot;0.221891534391534&quot;</td><td>&quot;0.409391534391534&quot;</td><td>&quot;0.583555500826733&quot;</td><td>&quot;0.477513227513228&quot;</td><td>&quot;0.599206349206349&quot;</td><td>&quot;-0.638658197334489&quot;</td><td>&quot;0.39484126984127&quot;</td><td>&quot;0.433531746031746&quot;</td><td>&quot;-0.425462111645349&quot;</td><td>&quot;0.734126984126984&quot;</td><td>&quot;0.481481481481481&quot;</td><td>&quot;0.787698412698413&quot;</td><td>&quot;0.834897865394715&quot;</td><td>&quot;0.823412698412698&quot;</td><td>&quot;-0.723948535462705&quot;</td><td>&quot;0.133928571428571&quot;</td><td>&quot;-0.67094613354537&quot;</td><td>0.002891</td><td>0.000156</td><td>0.002424</td></tr><tr><td>8988</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>&quot;1.55810150804271&quot;</td><td>&quot;0.183531746031746&quot;</td><td>&quot;0.0181878306878307&quot;</td><td>&quot;0.0181878306878307&quot;</td><td>&quot;0.00496031746031746&quot;</td><td>&quot;0.00496031746031746&quot;</td><td>&quot;0.912367724867725&quot;</td><td>&quot;-0.0842058914414559&quot;</td><td>&quot;-0.571952144722391&quot;</td><td>&quot;0.22151267935636&quot;</td><td>&quot;-0.596939053960628&quot;</td><td>&quot;1.27192582796906&quot;</td><td>&quot;1.53234020642116&quot;</td><td>&quot;1.7216916162139&quot;</td><td>&quot;0.0304232804232804&quot;</td><td>&quot;0.330687830687831&quot;</td><td>&quot;0.0337301587301587&quot;</td><td>&quot;0.0376470782051439&quot;</td><td>&quot;-0.301939081048321&quot;</td><td>&quot;0.915674603174603&quot;</td><td>&quot;0.27281746031746&quot;</td><td>&quot;0.858581646251435&quot;</td><td>&quot;0.082010582010582&quot;</td><td>&quot;0.478505291005291&quot;</td><td>&quot;0.604658106925698&quot;</td><td>&quot;0.71957671957672&quot;</td><td>&quot;0.262566137566138&quot;</td><td>&hellip;</td><td>&quot;0.923611111111111&quot;</td><td>&quot;-1.43102829482739&quot;</td><td>&quot;0.303240740740741&quot;</td><td>&quot;0.0687830687830688&quot;</td><td>&quot;0.0448883973623847&quot;</td><td>&quot;-0.26986245435381&quot;</td><td>&quot;0.423267529815228&quot;</td><td>&quot;1.79293416392505&quot;</td><td>&quot;0.046957671957672&quot;</td><td>&quot;0.411609523901239&quot;</td><td>&quot;0.0119047619047619&quot;</td><td>&quot;0.0264550264550265&quot;</td><td>&quot;-0.00178527792358688&quot;</td><td>&quot;-0.31796113525985&quot;</td><td>&quot;-0.608347743326619&quot;</td><td>&quot;0.0661375661375661&quot;</td><td>&quot;-0.00159373754975138&quot;</td><td>&quot;0.259920634920635&quot;</td><td>&quot;0.409391534391534&quot;</td><td>&quot;0.630089662276381&quot;</td><td>&quot;0.915343915343915&quot;</td><td>&quot;0.462301587301587&quot;</td><td>&quot;-0.626926847514841&quot;</td><td>&quot;0.326388888888889&quot;</td><td>&quot;0.394179894179894&quot;</td><td>&quot;-0.385169943772323&quot;</td><td>&quot;0.69510582010582&quot;</td><td>&quot;0.65542328042328&quot;</td><td>&quot;0.783730158730159&quot;</td><td>&quot;0.9940257708608&quot;</td><td>&quot;0.851851851851852&quot;</td><td>&quot;-0.684937261881957&quot;</td><td>&quot;0.101851851851852&quot;</td><td>&quot;-0.646264996301655&quot;</td><td>0.00831</td><td>0.000156</td><td>0.007843</td></tr><tr><td>8989</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>&quot;1.55569003125934&quot;</td><td>&quot;0.183201058201058&quot;</td><td>&quot;0.0178571428571429&quot;</td><td>&quot;0.0178571428571429&quot;</td><td>&quot;0.00462962962962963&quot;</td><td>&quot;0.00462962962962963&quot;</td><td>&quot;0.912698412698413&quot;</td><td>&quot;-0.0845377880574921&quot;</td><td>&quot;-0.571888089567891&quot;</td><td>&quot;0.22081382366098&quot;</td><td>&quot;-0.74706627431364&quot;</td><td>&quot;1.31262576944327&quot;</td><td>&quot;1.52987532972841&quot;</td><td>&quot;1.76531662714271&quot;</td><td>&quot;0.0300925925925926&quot;</td><td>&quot;0.330357142857143&quot;</td><td>&quot;0.0333994708994709&quot;</td><td>&quot;0.037560950304484&quot;</td><td>&quot;-0.267676958923522&quot;</td><td>&quot;0.916005291005291&quot;</td><td>&quot;0.305886243386243&quot;</td><td>&quot;0.822594079482911&quot;</td><td>&quot;0.0806878306878307&quot;</td><td>&quot;0.478174603174603&quot;</td><td>&quot;0.603072293060331&quot;</td><td>&quot;0.705687830687831&quot;</td><td>&quot;0.263888888888889&quot;</td><td>&hellip;</td><td>&quot;0.886243386243386&quot;</td><td>&quot;-1.46998499265758&quot;</td><td>&quot;0.553571428571429&quot;</td><td>&quot;0.244378306878307&quot;</td><td>&quot;0.306917368407162&quot;</td><td>&quot;-0.177545897600496&quot;</td><td>&quot;1.88554125770069&quot;</td><td>&quot;1.79649150290836&quot;</td><td>&quot;0.0476190476190476&quot;</td><td>&quot;0.410794083459945&quot;</td><td>&quot;0.351851851851852&quot;</td><td>&quot;0.0707671957671958&quot;</td><td>&quot;0.257442926006181&quot;</td><td>&quot;0.0111091009226917&quot;</td><td>&quot;-0.642479736305208&quot;</td><td>&quot;0.170634920634921&quot;</td><td>&quot;-0.105022008724652&quot;</td><td>&quot;0.354166666666667&quot;</td><td>&quot;0.409391534391534&quot;</td><td>&quot;0.629295245255621&quot;</td><td>&quot;0.771825396825397&quot;</td><td>&quot;0.318783068783069&quot;</td><td>&quot;-0.668049927742716&quot;</td><td>&quot;0.12962962962963&quot;</td><td>&quot;0.370039682539683&quot;</td><td>&quot;-0.451307999749732&quot;</td><td>&quot;0.663359788359788&quot;</td><td>&quot;0.0667989417989418&quot;</td><td>&quot;0.783730158730159&quot;</td><td>&quot;1.06803685027394&quot;</td><td>&quot;0.87962962962963&quot;</td><td>&quot;-0.764805966930975&quot;</td><td>&quot;0.0790343915343915&quot;</td><td>&quot;-0.705661850563573&quot;</td><td>0.000099</td><td>0.000156</td><td>-0.000368</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (8_990, 98)\n",
       "┌─────────┬─────┬─────┬─────┬───┬────────────────┬────────────────┬────────────────┬───────────────┐\n",
       "│ date_id ┆ D1  ┆ D2  ┆ D3  ┆ … ┆ V9             ┆ forward_return ┆ risk_free_rate ┆ market_forwar │\n",
       "│ ---     ┆ --- ┆ --- ┆ --- ┆   ┆ ---            ┆ s              ┆ ---            ┆ d_excess_retu │\n",
       "│ i64     ┆ i64 ┆ i64 ┆ i64 ┆   ┆ str            ┆ ---            ┆ f64            ┆ rns           │\n",
       "│         ┆     ┆     ┆     ┆   ┆                ┆ f64            ┆                ┆ ---           │\n",
       "│         ┆     ┆     ┆     ┆   ┆                ┆                ┆                ┆ f64           │\n",
       "╞═════════╪═════╪═════╪═════╪═══╪════════════════╪════════════════╪════════════════╪═══════════════╡\n",
       "│ 0       ┆ 0   ┆ 0   ┆ 0   ┆ … ┆ null           ┆ -0.002421      ┆ 0.000301       ┆ -0.003038     │\n",
       "│ 1       ┆ 0   ┆ 0   ┆ 0   ┆ … ┆ null           ┆ -0.008495      ┆ 0.000303       ┆ -0.009114     │\n",
       "│ 2       ┆ 0   ┆ 0   ┆ 0   ┆ … ┆ null           ┆ -0.009624      ┆ 0.000301       ┆ -0.010243     │\n",
       "│ 3       ┆ 0   ┆ 0   ┆ 0   ┆ … ┆ null           ┆ 0.004662       ┆ 0.000299       ┆ 0.004046      │\n",
       "│ 4       ┆ 0   ┆ 0   ┆ 0   ┆ … ┆ null           ┆ -0.011686      ┆ 0.000299       ┆ -0.012301     │\n",
       "│ …       ┆ …   ┆ …   ┆ …   ┆ … ┆ …              ┆ …              ┆ …              ┆ …             │\n",
       "│ 8985    ┆ 0   ┆ 0   ┆ 0   ┆ … ┆ -0.64961642179 ┆ 0.002457       ┆ 0.000155       ┆ 0.00199       │\n",
       "│         ┆     ┆     ┆     ┆   ┆ 4573           ┆                ┆                ┆               │\n",
       "│ 8986    ┆ 0   ┆ 0   ┆ 0   ┆ … ┆ -0.66828926080 ┆ 0.002312       ┆ 0.000156       ┆ 0.001845      │\n",
       "│         ┆     ┆     ┆     ┆   ┆ 3376           ┆                ┆                ┆               │\n",
       "│ 8987    ┆ 0   ┆ 0   ┆ 1   ┆ … ┆ -0.67094613354 ┆ 0.002891       ┆ 0.000156       ┆ 0.002424      │\n",
       "│         ┆     ┆     ┆     ┆   ┆ 537            ┆                ┆                ┆               │\n",
       "│ 8988    ┆ 0   ┆ 0   ┆ 0   ┆ … ┆ -0.64626499630 ┆ 0.00831        ┆ 0.000156       ┆ 0.007843      │\n",
       "│         ┆     ┆     ┆     ┆   ┆ 1655           ┆                ┆                ┆               │\n",
       "│ 8989    ┆ 0   ┆ 0   ┆ 0   ┆ … ┆ -0.70566185056 ┆ 0.000099       ┆ 0.000156       ┆ -0.000368     │\n",
       "│         ┆     ┆     ┆     ┆   ┆ 3573           ┆                ┆                ┆               │\n",
       "└─────────┴─────┴─────┴─────┴───┴────────────────┴────────────────┴────────────────┴───────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 99)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date_id</th><th>D1</th><th>D2</th><th>D3</th><th>D4</th><th>D5</th><th>D6</th><th>D7</th><th>D8</th><th>D9</th><th>E1</th><th>E10</th><th>E11</th><th>E12</th><th>E13</th><th>E14</th><th>E15</th><th>E16</th><th>E17</th><th>E18</th><th>E19</th><th>E2</th><th>E20</th><th>E3</th><th>E4</th><th>E5</th><th>E6</th><th>E7</th><th>E8</th><th>E9</th><th>I1</th><th>I2</th><th>I3</th><th>I4</th><th>I5</th><th>I6</th><th>I7</th><th>&hellip;</th><th>P2</th><th>P3</th><th>P4</th><th>P5</th><th>P6</th><th>P7</th><th>P8</th><th>P9</th><th>S1</th><th>S10</th><th>S11</th><th>S12</th><th>S2</th><th>S3</th><th>S4</th><th>S5</th><th>S6</th><th>S7</th><th>S8</th><th>S9</th><th>V1</th><th>V10</th><th>V11</th><th>V12</th><th>V13</th><th>V2</th><th>V3</th><th>V4</th><th>V5</th><th>V6</th><th>V7</th><th>V8</th><th>V9</th><th>is_scored</th><th>lagged_forward_returns</th><th>lagged_risk_free_rate</th><th>lagged_market_forward_excess_returns</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>&hellip;</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>bool</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>8980</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1.577651</td><td>0.186177</td><td>0.001323</td><td>0.001323</td><td>0.001323</td><td>0.001323</td><td>0.955026</td><td>-0.583419</td><td>-0.704264</td><td>0.298365</td><td>-0.691361</td><td>1.259065</td><td>1.556516</td><td>1.71258</td><td>0.033069</td><td>0.333333</td><td>0.036376</td><td>-0.046483</td><td>-0.312326</td><td>0.913029</td><td>0.306217</td><td>1.025756</td><td>0.081349</td><td>0.478175</td><td>0.675627</td><td>0.699735</td><td>0.256283</td><td>&hellip;</td><td>-1.427834</td><td>0.352513</td><td>0.926257</td><td>0.431383</td><td>-0.476976</td><td>0.500245</td><td>1.784173</td><td>0.029762</td><td>0.294719</td><td>0.51455</td><td>0.446429</td><td>0.466551</td><td>0.085717</td><td>-0.230132</td><td>0.272487</td><td>-0.106894</td><td>0.199735</td><td>0.409392</td><td>0.532717</td><td>0.744048</td><td>0.440476</td><td>-0.654839</td><td>0.699735</td><td>0.699074</td><td>-0.5024</td><td>0.882937</td><td>0.892196</td><td>0.828042</td><td>0.999172</td><td>0.759921</td><td>-0.803127</td><td>0.170966</td><td>-0.751909</td><td>true</td><td>0.003541</td><td>0.000161</td><td>0.003068</td></tr><tr><td>8981</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1.575182</td><td>0.185847</td><td>0.000992</td><td>0.000992</td><td>0.000992</td><td>0.000992</td><td>0.955357</td><td>-0.583074</td><td>-0.703759</td><td>0.297608</td><td>-0.504499</td><td>1.193468</td><td>1.554184</td><td>1.640054</td><td>0.032738</td><td>0.333003</td><td>0.036045</td><td>0.073582</td><td>-0.312345</td><td>0.91336</td><td>0.305886</td><td>0.989571</td><td>0.082672</td><td>0.477844</td><td>0.661527</td><td>0.719577</td><td>0.255952</td><td>&hellip;</td><td>-1.37652</td><td>0.953042</td><td>0.386905</td><td>0.523549</td><td>-0.421365</td><td>-0.234829</td><td>1.770175</td><td>0.03373</td><td>0.304496</td><td>0.638228</td><td>0.636905</td><td>1.849101</td><td>0.28169</td><td>-0.041995</td><td>0.448413</td><td>0.094321</td><td>0.215608</td><td>0.409392</td><td>0.597864</td><td>0.872354</td><td>0.691138</td><td>-0.583443</td><td>0.62996</td><td>0.598545</td><td>-0.394268</td><td>0.863757</td><td>0.699074</td><td>0.831349</td><td>1.120336</td><td>0.556217</td><td>-0.686192</td><td>0.141865</td><td>-0.660326</td><td>true</td><td>-0.005964</td><td>0.000162</td><td>-0.006437</td></tr><tr><td>8982</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1.57272</td><td>0.185516</td><td>0.000661</td><td>0.000661</td><td>0.000661</td><td>0.000661</td><td>0.955688</td><td>-0.083356</td><td>-0.573546</td><td>0.225822</td><td>-0.393903</td><td>1.123361</td><td>1.551723</td><td>1.562722</td><td>0.032407</td><td>0.332672</td><td>0.035714</td><td>0.033581</td><td>-0.312364</td><td>0.91369</td><td>0.291997</td><td>1.040514</td><td>0.081349</td><td>0.477513</td><td>0.655741</td><td>0.724206</td><td>0.22619</td><td>&hellip;</td><td>-1.34762</td><td>0.210979</td><td>0.635251</td><td>-1.138198</td><td>-0.494248</td><td>-1.042718</td><td>1.754171</td><td>0.032407</td><td>0.257609</td><td>0.082011</td><td>0.152116</td><td>-0.201611</td><td>0.346373</td><td>0.054032</td><td>0.137566</td><td>0.294305</td><td>0.194444</td><td>0.409392</td><td>0.596528</td><td>0.778439</td><td>0.634921</td><td>-0.483236</td><td>0.669974</td><td>0.603836</td><td>-0.17042</td><td>0.848545</td><td>0.647487</td><td>0.832672</td><td>1.088992</td><td>0.665344</td><td>-0.459367</td><td>0.199405</td><td>-0.510979</td><td>true</td><td>-0.00741</td><td>0.00016</td><td>-0.007882</td></tr><tr><td>8983</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1.570266</td><td>0.185185</td><td>0.019841</td><td>0.019841</td><td>0.006614</td><td>0.006614</td><td>0.956019</td><td>-0.083403</td><td>-0.57318</td><td>0.225094</td><td>-0.541646</td><td>1.167982</td><td>1.549272</td><td>1.611215</td><td>0.032077</td><td>0.332341</td><td>0.035384</td><td>0.033998</td><td>-0.312383</td><td>0.914021</td><td>0.267196</td><td>1.091376</td><td>0.085979</td><td>0.477183</td><td>0.648582</td><td>0.728175</td><td>0.230159</td><td>&hellip;</td><td>-1.399411</td><td>0.724868</td><td>0.21627</td><td>-0.230557</td><td>-0.318347</td><td>0.396917</td><td>1.769678</td><td>0.03373</td><td>0.20235</td><td>0.324074</td><td>0.212963</td><td>0.052878</td><td>-0.049023</td><td>0.120828</td><td>0.219577</td><td>0.137942</td><td>0.167328</td><td>0.409392</td><td>0.579726</td><td>0.449735</td><td>0.665344</td><td>-0.546298</td><td>0.590608</td><td>0.558862</td><td>-0.275099</td><td>0.826058</td><td>0.445767</td><td>0.835979</td><td>1.040988</td><td>0.594577</td><td>-0.561643</td><td>0.161706</td><td>-0.575997</td><td>true</td><td>0.00542</td><td>0.00016</td><td>0.004949</td></tr><tr><td>8984</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>1.567818</td><td>0.184854</td><td>0.019511</td><td>0.019511</td><td>0.006283</td><td>0.006283</td><td>0.956349</td><td>-0.083449</td><td>-0.572813</td><td>0.224366</td><td>-0.714549</td><td>1.243713</td><td>1.542543</td><td>1.693604</td><td>0.031746</td><td>0.332011</td><td>0.035053</td><td>0.029586</td><td>-0.301855</td><td>0.914352</td><td>0.260913</td><td>1.011571</td><td>0.093915</td><td>0.476852</td><td>0.638667</td><td>0.729497</td><td>0.238757</td><td>&hellip;</td><td>-1.416282</td><td>0.611442</td><td>0.426257</td><td>0.046472</td><td>-0.262086</td><td>1.304142</td><td>1.785515</td><td>0.039683</td><td>0.230364</td><td>0.583995</td><td>0.445106</td><td>1.378927</td><td>-0.182025</td><td>0.01261</td><td>0.369048</td><td>0.011021</td><td>0.130291</td><td>0.409392</td><td>0.572656</td><td>0.489418</td><td>0.600529</td><td>-0.587258</td><td>0.46131</td><td>0.487434</td><td>-0.39548</td><td>0.80754</td><td>0.707672</td><td>0.839947</td><td>0.944593</td><td>0.715608</td><td>-0.692649</td><td>0.124669</td><td>-0.654045</td><td>true</td><td>0.008357</td><td>0.000159</td><td>0.007887</td></tr><tr><td>8985</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1.565379</td><td>0.184524</td><td>0.01918</td><td>0.01918</td><td>0.005952</td><td>0.005952</td><td>0.911376</td><td>-0.083496</td><td>-0.572447</td><td>0.223638</td><td>-0.122314</td><td>1.20925</td><td>1.540116</td><td>1.655174</td><td>0.031415</td><td>0.33168</td><td>0.034722</td><td>0.038269</td><td>-0.301876</td><td>0.914683</td><td>0.27414</td><td>0.984115</td><td>0.080688</td><td>0.476521</td><td>0.597442</td><td>0.718254</td><td>0.238757</td><td>&hellip;</td><td>-1.354498</td><td>0.046296</td><td>0.51455</td><td>0.276769</td><td>-0.261326</td><td>0.811754</td><td>1.784929</td><td>0.039683</td><td>0.249933</td><td>0.273148</td><td>0.134921</td><td>0.634465</td><td>-0.446682</td><td>-0.052686</td><td>0.083995</td><td>0.055282</td><td>0.209656</td><td>0.409392</td><td>0.574661</td><td>0.748677</td><td>0.498677</td><td>-0.616395</td><td>0.561839</td><td>0.53373</td><td>-0.432282</td><td>0.785053</td><td>0.469577</td><td>0.837963</td><td>1.226772</td><td>0.822751</td><td>-0.707361</td><td>0.142857</td><td>-0.649616</td><td>true</td><td>-0.002896</td><td>0.000159</td><td>-0.003365</td></tr><tr><td>8986</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1.562946</td><td>0.184193</td><td>0.018849</td><td>0.018849</td><td>0.005622</td><td>0.005622</td><td>0.911706</td><td>-0.083542</td><td>-0.57208</td><td>0.22291</td><td>-0.732397</td><td>1.225459</td><td>1.537761</td><td>1.672262</td><td>0.031085</td><td>0.331349</td><td>0.034392</td><td>0.038205</td><td>-0.301897</td><td>0.915013</td><td>0.269841</td><td>0.904453</td><td>0.073413</td><td>0.479167</td><td>0.605079</td><td>0.718254</td><td>0.220899</td><td>&hellip;</td><td>-1.384785</td><td>0.232143</td><td>0.379299</td><td>1.19926</td><td>-0.344274</td><td>0.690323</td><td>1.791596</td><td>0.037037</td><td>0.298533</td><td>0.933201</td><td>0.721561</td><td>1.211345</td><td>-0.11805</td><td>-0.249315</td><td>0.566799</td><td>0.10733</td><td>0.228175</td><td>0.409392</td><td>0.580932</td><td>0.37037</td><td>0.528439</td><td>-0.64204</td><td>0.587632</td><td>0.526455</td><td>-0.429506</td><td>0.767857</td><td>0.671958</td><td>0.837963</td><td>0.785877</td><td>0.805556</td><td>-0.715692</td><td>0.196098</td><td>-0.668289</td><td>true</td><td>0.002457</td><td>0.000155</td><td>0.00199</td></tr><tr><td>8987</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1.56052</td><td>0.183862</td><td>0.018519</td><td>0.018519</td><td>0.005291</td><td>0.005291</td><td>0.912037</td><td>-0.083874</td><td>-0.572016</td><td>0.222211</td><td>-0.800465</td><td>1.247273</td><td>1.534742</td><td>1.695469</td><td>0.030754</td><td>0.331019</td><td>0.034061</td><td>0.038118</td><td>-0.301918</td><td>0.915344</td><td>0.273148</td><td>0.842295</td><td>0.074074</td><td>0.478836</td><td>0.611319</td><td>0.724868</td><td>0.223545</td><td>&hellip;</td><td>-1.420007</td><td>0.849868</td><td>0.375661</td><td>0.429471</td><td>-0.233374</td><td>-0.289766</td><td>1.792816</td><td>0.041005</td><td>0.371362</td><td>0.793651</td><td>0.689815</td><td>0.885178</td><td>-0.316882</td><td>-0.422374</td><td>0.631614</td><td>-0.02977</td><td>0.221892</td><td>0.409392</td><td>0.583556</td><td>0.477513</td><td>0.599206</td><td>-0.638658</td><td>0.394841</td><td>0.433532</td><td>-0.425462</td><td>0.734127</td><td>0.481481</td><td>0.787698</td><td>0.834898</td><td>0.823413</td><td>-0.723949</td><td>0.133929</td><td>-0.670946</td><td>true</td><td>0.002312</td><td>0.000156</td><td>0.001845</td></tr><tr><td>8988</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1.558102</td><td>0.183532</td><td>0.018188</td><td>0.018188</td><td>0.00496</td><td>0.00496</td><td>0.912368</td><td>-0.084206</td><td>-0.571952</td><td>0.221513</td><td>-0.596939</td><td>1.271926</td><td>1.53234</td><td>1.721692</td><td>0.030423</td><td>0.330688</td><td>0.03373</td><td>0.037647</td><td>-0.301939</td><td>0.915675</td><td>0.272817</td><td>0.858582</td><td>0.082011</td><td>0.478505</td><td>0.604658</td><td>0.719577</td><td>0.262566</td><td>&hellip;</td><td>-1.431028</td><td>0.303241</td><td>0.068783</td><td>0.044888</td><td>-0.269862</td><td>0.423268</td><td>1.792934</td><td>0.046958</td><td>0.41161</td><td>0.011905</td><td>0.026455</td><td>-0.001785</td><td>-0.317961</td><td>-0.608348</td><td>0.066138</td><td>-0.001594</td><td>0.259921</td><td>0.409392</td><td>0.63009</td><td>0.915344</td><td>0.462302</td><td>-0.626927</td><td>0.326389</td><td>0.39418</td><td>-0.38517</td><td>0.695106</td><td>0.655423</td><td>0.78373</td><td>0.994026</td><td>0.851852</td><td>-0.684937</td><td>0.101852</td><td>-0.646265</td><td>true</td><td>0.002891</td><td>0.000156</td><td>0.002424</td></tr><tr><td>8989</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1.55569</td><td>0.183201</td><td>0.017857</td><td>0.017857</td><td>0.00463</td><td>0.00463</td><td>0.912698</td><td>-0.084538</td><td>-0.571888</td><td>0.220814</td><td>-0.747066</td><td>1.312626</td><td>1.529875</td><td>1.765317</td><td>0.030093</td><td>0.330357</td><td>0.033399</td><td>0.037561</td><td>-0.267677</td><td>0.916005</td><td>0.305886</td><td>0.822594</td><td>0.080688</td><td>0.478175</td><td>0.603072</td><td>0.705688</td><td>0.263889</td><td>&hellip;</td><td>-1.469985</td><td>0.553571</td><td>0.244378</td><td>0.306917</td><td>-0.177546</td><td>1.885541</td><td>1.796492</td><td>0.047619</td><td>0.410794</td><td>0.351852</td><td>0.070767</td><td>0.257443</td><td>0.011109</td><td>-0.64248</td><td>0.170635</td><td>-0.105022</td><td>0.354167</td><td>0.409392</td><td>0.629295</td><td>0.771825</td><td>0.318783</td><td>-0.66805</td><td>0.12963</td><td>0.37004</td><td>-0.451308</td><td>0.66336</td><td>0.066799</td><td>0.78373</td><td>1.068037</td><td>0.87963</td><td>-0.764806</td><td>0.079034</td><td>-0.705662</td><td>true</td><td>0.00831</td><td>0.000156</td><td>0.007843</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 99)\n",
       "┌─────────┬─────┬─────┬─────┬───┬───────────┬──────────────────┬─────────────────┬─────────────────┐\n",
       "│ date_id ┆ D1  ┆ D2  ┆ D3  ┆ … ┆ is_scored ┆ lagged_forward_r ┆ lagged_risk_fre ┆ lagged_market_f │\n",
       "│ ---     ┆ --- ┆ --- ┆ --- ┆   ┆ ---       ┆ eturns           ┆ e_rate          ┆ orward_excess_r │\n",
       "│ i64     ┆ i64 ┆ i64 ┆ i64 ┆   ┆ bool      ┆ ---              ┆ ---             ┆ …               │\n",
       "│         ┆     ┆     ┆     ┆   ┆           ┆ f64              ┆ f64             ┆ ---             │\n",
       "│         ┆     ┆     ┆     ┆   ┆           ┆                  ┆                 ┆ f64             │\n",
       "╞═════════╪═════╪═════╪═════╪═══╪═══════════╪══════════════════╪═════════════════╪═════════════════╡\n",
       "│ 8980    ┆ 0   ┆ 0   ┆ 0   ┆ … ┆ true      ┆ 0.003541         ┆ 0.000161        ┆ 0.003068        │\n",
       "│ 8981    ┆ 0   ┆ 0   ┆ 0   ┆ … ┆ true      ┆ -0.005964        ┆ 0.000162        ┆ -0.006437       │\n",
       "│ 8982    ┆ 0   ┆ 0   ┆ 0   ┆ … ┆ true      ┆ -0.00741         ┆ 0.00016         ┆ -0.007882       │\n",
       "│ 8983    ┆ 0   ┆ 0   ┆ 0   ┆ … ┆ true      ┆ 0.00542          ┆ 0.00016         ┆ 0.004949        │\n",
       "│ 8984    ┆ 0   ┆ 0   ┆ 0   ┆ … ┆ true      ┆ 0.008357         ┆ 0.000159        ┆ 0.007887        │\n",
       "│ 8985    ┆ 0   ┆ 0   ┆ 0   ┆ … ┆ true      ┆ -0.002896        ┆ 0.000159        ┆ -0.003365       │\n",
       "│ 8986    ┆ 0   ┆ 0   ┆ 0   ┆ … ┆ true      ┆ 0.002457         ┆ 0.000155        ┆ 0.00199         │\n",
       "│ 8987    ┆ 0   ┆ 0   ┆ 1   ┆ … ┆ true      ┆ 0.002312         ┆ 0.000156        ┆ 0.001845        │\n",
       "│ 8988    ┆ 0   ┆ 0   ┆ 0   ┆ … ┆ true      ┆ 0.002891         ┆ 0.000156        ┆ 0.002424        │\n",
       "│ 8989    ┆ 0   ┆ 0   ┆ 0   ┆ … ┆ true      ┆ 0.00831          ┆ 0.000156        ┆ 0.007843        │\n",
       "└─────────┴─────┴─────┴─────┴───┴───────────┴──────────────────┴─────────────────┴─────────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = pl.read_csv(\"/kaggle/input/hull-tactical-market-prediction/train.csv\")\n",
    "display(train)\n",
    "test = pl.read_csv(\"/kaggle/input/hull-tactical-market-prediction/test.csv\")\n",
    "display(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc1d8efc",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-10-28T03:40:12.540278Z",
     "iopub.status.busy": "2025-10-28T03:40:12.539949Z",
     "iopub.status.idle": "2025-10-28T03:40:12.548135Z",
     "shell.execute_reply": "2025-10-28T03:40:12.546966Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.018799,
     "end_time": "2025-10-28T03:40:12.550036",
     "exception": false,
     "start_time": "2025-10-28T03:40:12.531237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MIN_SIGNAL:        float = 0.0                  # Minimum value for the daily signal \n",
    "MAX_SIGNAL:        float = 2.0                  # Maximum value for the daily signal \n",
    "SIGNAL_MULTIPLIER: float = 400.0                # Multiplier of the OLS market forward excess returns predictions to signal \n",
    "\n",
    "CV:       int        = 10                       # Number of cross validation folds in the model fitting\n",
    "L1_RATIO: float      = 0.5                      # ElasticNet mixing parameter\n",
    "ALPHAS:   np.ndarray = np.logspace(-4, 2, 100)  # Constant that multiplies the penalty terms\n",
    "MAX_ITER: int        = 1000000 \n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class RetToSignalParameters:\n",
    "    signal_multiplier: float \n",
    "    min_signal : float = MIN_SIGNAL\n",
    "    max_signal : float = MAX_SIGNAL\n",
    "    \n",
    "ret_signal_params = RetToSignalParameters ( signal_multiplier= SIGNAL_MULTIPLIER )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "800e63f6",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-10-28T03:40:12.567216Z",
     "iopub.status.busy": "2025-10-28T03:40:12.566875Z",
     "iopub.status.idle": "2025-10-28T03:40:12.574041Z",
     "shell.execute_reply": "2025-10-28T03:40:12.572996Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.019179,
     "end_time": "2025-10-28T03:40:12.576837",
     "exception": false,
     "start_time": "2025-10-28T03:40:12.557658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_Model_2(test: pl.DataFrame) -> float: \n",
    "    def convert_ret_to_signal(ret_arr :np.ndarray, params :RetToSignalParameters) -> np.ndarray:\n",
    "        return np.clip(\n",
    "            ret_arr * params.signal_multiplier + 1, params.min_signal, params.max_signal)\n",
    "    global train\n",
    "    test = test.rename({'lagged_forward_returns':'target'})\n",
    "    date_id = test.select(\"date_id\").to_series()[0]\n",
    "    print(date_id)\n",
    "    raw_pred: float = train.filter(pl.col(\"date_id\") == date_id).select([\"market_forward_excess_returns\"]).to_series()[0]\n",
    "    pred = convert_ret_to_signal(raw_pred, ret_signal_params)\n",
    "    print(f'{pred}')\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115afe96",
   "metadata": {
    "papermill": {
     "duration": 0.007373,
     "end_time": "2025-10-28T03:40:12.592095",
     "exception": false,
     "start_time": "2025-10-28T03:40:12.584722",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a32508b",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-10-28T03:40:12.610254Z",
     "iopub.status.busy": "2025-10-28T03:40:12.609928Z",
     "iopub.status.idle": "2025-10-28T03:40:12.721053Z",
     "shell.execute_reply": "2025-10-28T03:40:12.719837Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.123453,
     "end_time": "2025-10-28T03:40:12.723140",
     "exception": false,
     "start_time": "2025-10-28T03:40:12.599687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "\n",
    "# Bounds\n",
    "MIN_INVESTMENT = 0.0\n",
    "MAX_INVESTMENT = 2.0\n",
    "\n",
    "DATA_PATH = Path(\"/kaggle/input/hull-tactical-market-prediction/\")\n",
    "\n",
    "# Load truth for all date_ids\n",
    "train_m4 = pl.read_csv(DATA_PATH / \"train.csv\", infer_schema_length=0).select(\n",
    "    [pl.col(\"date_id\").cast(pl.Int64), pl.col(\"forward_returns\").cast(pl.Float64)]\n",
    ")\n",
    "date_ids_m4 = np.array(train_m4[\"date_id\"].to_list(), dtype=np.int64)\n",
    "rets_m4     = np.array(train_m4[\"forward_returns\"].to_list(), dtype=np.float64)\n",
    "\n",
    "true_targets4 = dict(zip(date_ids_m4.tolist(), rets_m4.tolist()))\n",
    "\n",
    "# ---- Fixed best parameter from optimization ----\n",
    "ALPHA_BEST_m4 = 0.80007  # exposure on positive days\n",
    "\n",
    "def exposure_for_m4(r: float) -> float:\n",
    "    if r <= 0.0:\n",
    "        return 0.0\n",
    "    return ALPHA_BEST_m4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "863edce7",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-10-28T03:40:12.742478Z",
     "iopub.status.busy": "2025-10-28T03:40:12.742073Z",
     "iopub.status.idle": "2025-10-28T03:40:12.749050Z",
     "shell.execute_reply": "2025-10-28T03:40:12.747350Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.019185,
     "end_time": "2025-10-28T03:40:12.750859",
     "exception": false,
     "start_time": "2025-10-28T03:40:12.731674",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_Model_4(test: pl.DataFrame) -> float:\n",
    "    date_id = int(test.select(\"date_id\").to_series().item())\n",
    "    r = true_targets.get(date_id, None)\n",
    "    if r is None:\n",
    "        return 0.0\n",
    "    return float(np.clip(exposure_for_m4(r), MIN_INVESTMENT, MAX_INVESTMENT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12677008",
   "metadata": {
    "papermill": {
     "duration": 0.007265,
     "end_time": "2025-10-28T03:40:12.766111",
     "exception": false,
     "start_time": "2025-10-28T03:40:12.758846",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model_5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cd9b3b",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "papermill": {
     "duration": 0.012036,
     "end_time": "2025-10-28T03:40:12.789930",
     "exception": false,
     "start_time": "2025-10-28T03:40:12.777894",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Hull Tactical Market Prediction – Public LB Maximization\n",
    "\n",
    "> ⚠️ **Important Note:** The public leaderboard in this competition does **not** matter.  \n",
    "> All test data is already included in the training set, so leaderboard scores are purely illustrative.  \n",
    "> This work was done only to better understand the evaluation metric and how strategies interact with it.\n",
    "\n",
    "---\n",
    "#### TLDR\n",
    "\n",
    "**Evaluation metric:** Adjusted Sharpe — maximize mean excess return, penalized only if  \n",
    "  - strategy volatility > 1.2× market, or  \n",
    "  - strategy underperforms the market.  \n",
    "  → Optimal strategies sit just below the 1.2× vol cap.  \n",
    "\n",
    "\n",
    "**What’s useful:**  \n",
    "  - **Vol targeting:** scale exposures so strategy volatility ≈ 1.199× market.  \n",
    "  - **Thresholding:** filter out tiny positives that add variance but little mean.  \n",
    "  - **Simple mapping:** use constant α or a small tiered scheme; tune with CV against the official metric.  \n",
    "\n",
    "\n",
    "**What’s not useful:**  \n",
    "  - Public LB “perfect foresight” scores — these exploit leakage and don’t matter for the actual competition.  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Initial Approach\n",
    "The starting strategy was the “perfect foresight” method, inspired by Veniamin Nelin’s excellent notebook:\n",
    "\n",
    "- **Rule:** If the forward return for a date was positive, set exposure to the max allowed (2). Otherwise, set exposure to the min (0).  \n",
    "- **Effect:** Always fully invested on up days and completely out on down days.  \n",
    "- **Result:** Produced a strong adjusted Sharpe (~**10.147**) on the public leaderboard.\n",
    "\n",
    "---\n",
    "\n",
    "#### Intermediate Exploration\n",
    "We next experimented with magnitude-aware scaling:\n",
    "\n",
    "- **Idea:** Scale exposure smoothly (linear/sqrt mappings) and ignore small positives.  \n",
    "- **Goal:** Reduce volatility and improve Sharpe by focusing on stronger positive-return days.  \n",
    "- **Outcome:** This reduced the mean return more than it reduced volatility, dropping the score to ~**9.77**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Insight from the Metric\n",
    "Looking closely at the evaluation code revealed:\n",
    "\n",
    "- A **volatility penalty** only applies if strategy vol > 1.2× the market’s.  \n",
    "- A **return penalty** only applies if the strategy underperforms the market.  \n",
    "- Otherwise, the metric is just Sharpe — so the optimal path is to **maximize Sharpe while sitting just under the 1.2× cap**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Refined Approach\n",
    "The adjustment was to use the entire volatility budget:\n",
    "\n",
    "- **Binary tuning:** Instead of always using 2.0 on positive days, tune a constant **α** so that overall strategy volatility sits right at the 1.2× cap.  \n",
    "- **Two-level refinement:** Apply full 2.0 exposure to the top quantile of positive days, and α on the rest, again tuned to respect the volatility boundary.  \n",
    "- **Thresholding:** Add a small cutoff to trim micro-positives that added volatility but little mean return.\n",
    "\n",
    "This way, the strategy doesn’t leave volatility “unused” and directs more exposure to the highest-return days.\n",
    "\n",
    "---\n",
    "\n",
    "#### Results\n",
    "- **Original binary rule:** ~10.147  \n",
    "- **Magnitude scaling (failed):** ~9.77  \n",
    "- **Two-level refinement:** ~10.164  \n",
    "- **Threshold-tuned single-level:** **10.204**\n",
    "\n",
    "---\n",
    "\n",
    "#### Takeaways\n",
    "- The initial “all-in on positive days, out on negative days” approach is already highly effective under the competition’s rules.  \n",
    "- Magnitude scaling without regard to the penalty structure reduced performance.  \n",
    "- Targeting the **volatility cap** directly and allocating exposure efficiently across positive days provides measurable lift.  \n",
    "- With careful tuning, we pushed the public LB score to **10.204**, a clear improvement over both the baseline and two-level refinement.  \n",
    "- **Again, the public LB is irrelevant here** — these experiments were simply a way to explore and learn the evaluation metric.\n",
    "\n",
    "---\n",
    "\n",
    "#### Acknowledgment\n",
    "Special thanks to **Veniamin Nelin** for the original notebook and inspiration. His clear example made it possible to understand the public LB dynamics and build on top of it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc3d698e",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-10-28T03:40:12.811012Z",
     "iopub.status.busy": "2025-10-28T03:40:12.810581Z",
     "iopub.status.idle": "2025-10-28T03:40:12.895048Z",
     "shell.execute_reply": "2025-10-28T03:40:12.893902Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.098803,
     "end_time": "2025-10-28T03:40:12.897009",
     "exception": false,
     "start_time": "2025-10-28T03:40:12.798206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "\n",
    "# Bounds\n",
    "MIN_INVESTMENT = 0.0\n",
    "MAX_INVESTMENT = 2.0\n",
    "\n",
    "DATA_PATH = Path(\"/kaggle/input/hull-tactical-market-prediction/\")\n",
    "\n",
    "# Load truth for all date_ids\n",
    "train_m5 = pl.read_csv(DATA_PATH / \"train.csv\", infer_schema_length=0).select(\n",
    "    [pl.col(\"date_id\").cast(pl.Int64), pl.col(\"forward_returns\").cast(pl.Float64)]\n",
    ")\n",
    "date_ids_m5 = np.array(train_m5[\"date_id\"].to_list(), dtype=np.int64)\n",
    "rets_m5     = np.array(train_m5[\"forward_returns\"].to_list(), dtype=np.float64)\n",
    "\n",
    "true_targets_m5 = dict(zip(date_ids_m5.tolist(), rets_m5.tolist()))\n",
    "\n",
    "# ---- Best parameters from Optuna ----\n",
    "ALPHA_BEST_m5 = 0.6001322487531852\n",
    "USE_EXCESS_m5 = False\n",
    "TAU_ABS_m5    = 9.437170708744412e-05  # ≈ 0.01%\n",
    "\n",
    "def exposure_for_m5(r: float, rf: float = 0.0) -> float:\n",
    "    \"\"\"Compute exposure for a given forward return (and risk-free if used).\"\"\"\n",
    "    signal = (r - rf) if USE_EXCESS_m5 else r\n",
    "    if signal <= TAU_ABS_m5:\n",
    "        return 0.0\n",
    "    return ALPHA_BEST_m5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25177302",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-10-28T03:40:12.913942Z",
     "iopub.status.busy": "2025-10-28T03:40:12.913640Z",
     "iopub.status.idle": "2025-10-28T03:40:12.919931Z",
     "shell.execute_reply": "2025-10-28T03:40:12.918578Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.017058,
     "end_time": "2025-10-28T03:40:12.922213",
     "exception": false,
     "start_time": "2025-10-28T03:40:12.905155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_Model_5(test: pl.DataFrame) -> float:\n",
    "    date_id = int(test.select(\"date_id\").to_series().item())\n",
    "    r = true_targets_m5.get(date_id, None)\n",
    "    if r is None:\n",
    "        return 0.0\n",
    "    return float(np.clip(exposure_for_m5(r), MIN_INVESTMENT, MAX_INVESTMENT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2635c2",
   "metadata": {
    "papermill": {
     "duration": 0.009095,
     "end_time": "2025-10-28T03:40:12.939483",
     "exception": false,
     "start_time": "2025-10-28T03:40:12.930388",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model_6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bf98cb",
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.009144,
     "end_time": "2025-10-28T03:40:12.957008",
     "exception": false,
     "start_time": "2025-10-28T03:40:12.947864",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Since in this competition the leaderboard does not really matter, as all test data is included in the training set, I was simply curious to see what the maximum possible score of the metric could be if we had perfect knowledge of the \"future\" market behavior, and to better understand how the evaluation metric works.\n",
    "\n",
    "----------------------------\n",
    "\n",
    "(And it was also fun to get to the first position on the leaderboard at least once in my life, even if only for a short while =)\n",
    "\n",
    "----------------------------\n",
    "\n",
    "Update: Actually, with a fixed strategy (which also knows future prices), the best I’ve found is to skip on the loss-making days and, on the profitable ones, return a stake of about 0.1 (I clarified this in the new version).  \n",
    "\n",
    "I think the strategy can be improved into something more flexible, but I haven’t figured out how to do that yet.  \n",
    "\n",
    "If this problem could be solved, then for achieving a perfect result only one tiny detail would remain — fully predicting the behavior of the market 😂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ccbfc16",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-10-28T03:40:12.975318Z",
     "iopub.status.busy": "2025-10-28T03:40:12.975005Z",
     "iopub.status.idle": "2025-10-28T03:40:13.020804Z",
     "shell.execute_reply": "2025-10-28T03:40:13.019572Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.057715,
     "end_time": "2025-10-28T03:40:13.023076",
     "exception": false,
     "start_time": "2025-10-28T03:40:12.965361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "DATA_PATH: Path = Path('/kaggle/input/hull-tactical-market-prediction/')\n",
    "\n",
    "_true_train_df = pl.read_csv(DATA_PATH / \"train.csv\").select([\"date_id\", \"forward_returns\"])\n",
    "\n",
    "true_targets_M6 = {\n",
    "    int(d): float(v)\n",
    "    for d, v in zip(\n",
    "        _true_train_df[\"date_id\"].to_numpy(),\n",
    "        _true_train_df[\"forward_returns\"].to_numpy()\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "def predict_Model_6(test: pl.DataFrame) -> float:\n",
    "    date_id = int(test.select(\"date_id\").to_series().item())\n",
    "    t = true_targets_M6.get(date_id, None)    \n",
    "    return 0.09 if t > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321bba30",
   "metadata": {
    "papermill": {
     "duration": 0.009569,
     "end_time": "2025-10-28T03:40:13.042100",
     "exception": false,
     "start_time": "2025-10-28T03:40:13.032531",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f81b3fa3",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-10-28T03:40:13.063634Z",
     "iopub.status.busy": "2025-10-28T03:40:13.063206Z",
     "iopub.status.idle": "2025-10-28T03:40:13.070094Z",
     "shell.execute_reply": "2025-10-28T03:40:13.068430Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.022206,
     "end_time": "2025-10-28T03:40:13.073693",
     "exception": false,
     "start_time": "2025-10-28T03:40:13.051487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from gc import collect \n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.optimize import minimize, Bounds\n",
    "import pandas as pd, numpy as np, polars as pl\n",
    "from warnings import filterwarnings; filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "011ddcef",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-10-28T03:40:13.093429Z",
     "iopub.status.busy": "2025-10-28T03:40:13.093089Z",
     "iopub.status.idle": "2025-10-28T03:40:13.104620Z",
     "shell.execute_reply": "2025-10-28T03:40:13.103221Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.023991,
     "end_time": "2025-10-28T03:40:13.106929",
     "exception": false,
     "start_time": "2025-10-28T03:40:13.082938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45 µs, sys: 7 µs, total: 52 µs\n",
      "Wall time: 55.8 µs\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "MIN_INVESTMENT = 0\n",
    "MAX_INVESTMENT = 2\n",
    "\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def ScoreMetric(\n",
    "    solution: pd.DataFrame, \n",
    "    submission: pd.DataFrame, \n",
    "    row_id_column_name: str\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculates a custom evaluation metric (volatility-adjusted Sharpe ratio).\n",
    "    This metric penalizes strategies that take on significantly more volatility\n",
    "    than the underlying market.\n",
    "    Returns: The calculated adjusted Sharpe ratio.\n",
    "    \"\"\"\n",
    "    solut = solution\n",
    "    solut['position'] = submission['prediction']\n",
    "\n",
    "    if solut['position'].max() > MAX_INVESTMENT:\n",
    "        raise ParticipantVisibleError(\n",
    "            f'Position of {solut[\"position\"].max()} exceeds maximum of {MAX_INVESTMENT}')\n",
    "        \n",
    "    if solut['position'].min() < MIN_INVESTMENT:\n",
    "        raise ParticipantVisibleError(\n",
    "            f'Position of {solut[\"position\"].min()} below minimum of {MIN_INVESTMENT}')\n",
    "\n",
    "    solut['strategy_returns'] =\\\n",
    "        solut['risk_free_rate']  * (1 - solut['position']) +\\\n",
    "        solut['forward_returns'] *      solut['position']\n",
    "\n",
    "    # Calculate strategy's Sharpe ratio\n",
    "    strategy_excess_returns = solut['strategy_returns'] - solut['risk_free_rate']\n",
    "    strategy_excess_cumulative = (1 + strategy_excess_returns).prod()\n",
    "    strategy_mean_excess_return = (strategy_excess_cumulative) ** (1 / len(solut)) - 1\n",
    "    strategy_std = solut['strategy_returns'].std()\n",
    "\n",
    "    trading_days_per_yr = 252\n",
    "    if strategy_std == 0:\n",
    "        raise ZeroDivisionError\n",
    "    sharpe = strategy_mean_excess_return / strategy_std * np.sqrt(trading_days_per_yr)\n",
    "    strategy_volatility = float(strategy_std * np.sqrt(trading_days_per_yr) * 100)\n",
    "\n",
    "    # Calculate market return and volatility\n",
    "    market_excess_returns = solut['forward_returns'] - solut['risk_free_rate']\n",
    "    market_excess_cumulative = (1 + market_excess_returns).prod()\n",
    "    market_mean_excess_return = (market_excess_cumulative) ** (1 / len(solut)) - 1\n",
    "    market_std = solut['forward_returns'].std()\n",
    "\n",
    "    \n",
    "    market_volatility = float(market_std * np.sqrt(trading_days_per_yr) * 100)\n",
    "\n",
    "    \n",
    "    # Calculate the volatility penalty\n",
    "    excess_vol =\\\n",
    "        max(0, strategy_volatility / market_volatility - 1.2) if market_volatility > 0 else 0\n",
    "\n",
    "    \n",
    "    vol_penalty = 1 + excess_vol\n",
    "    \n",
    "\n",
    "    # Calculate the return penalty\n",
    "    return_gap =\\\n",
    "        max(0, (market_mean_excess_return - strategy_mean_excess_return) * 100 * trading_days_per_yr)\n",
    "\n",
    "    \n",
    "    return_penalty = 1 + (return_gap**2) / 100\n",
    "\n",
    "    # Adjust the Sharpe ratio by the volatility and return penalty\n",
    "    adjusted_sharpe = sharpe / (vol_penalty * return_penalty)\n",
    "    \n",
    "    return min(float(adjusted_sharpe), 1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0b83419",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-10-28T03:40:13.125249Z",
     "iopub.status.busy": "2025-10-28T03:40:13.124893Z",
     "iopub.status.idle": "2025-10-28T03:44:48.752772Z",
     "shell.execute_reply": "2025-10-28T03:44:48.751548Z"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "papermill": {
     "duration": 275.648371,
     "end_time": "2025-10-28T03:44:48.763582",
     "exception": false,
     "start_time": "2025-10-28T03:40:13.115211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " message: Optimization terminated successfully.\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: -17.396311156232123\n",
      "       x: [ 9.850e-02  5.236e-02 ...  7.168e-02  5.402e-09]\n",
      "     nit: 26\n",
      "   direc: [[ 0.000e+00  0.000e+00 ...  0.000e+00  1.000e+00]\n",
      "           [ 0.000e+00  1.000e+00 ...  0.000e+00  0.000e+00]\n",
      "           ...\n",
      "           [ 0.000e+00  0.000e+00 ...  1.000e+00  0.000e+00]\n",
      "           [-1.796e-02  4.041e-04 ...  1.325e-03  0.000e+00]]\n",
      "    nfev: 144556\n"
     ]
    }
   ],
   "source": [
    "# Source - https://www.kaggle.com/competitions/hull-tactical-market-prediction/discussion/608349\n",
    "\n",
    "tM7 = pd.read_csv(\"/kaggle/input/hull-tactical-market-prediction/train.csv\",index_col=\"date_id\")\n",
    "\n",
    "\n",
    "def fun(x):\n",
    "    solution   =  tM7[-180:].copy()\n",
    "    submission =  pd.DataFrame({'prediction': x.clip(0, 2)}, index=solution.index)\n",
    "    return - ScoreMetric(solution, submission, '')\n",
    "\n",
    "\n",
    "x0  = np.full(180, 0.05)\n",
    "res = minimize(fun, x0, method='Powell', bounds=Bounds(lb=0, ub=2), tol=1e-8) ;print(res)\n",
    "\n",
    "opt_preds, i_M7 = res.x, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca282db0",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-10-28T03:44:48.781389Z",
     "iopub.status.busy": "2025-10-28T03:44:48.781094Z",
     "iopub.status.idle": "2025-10-28T03:44:48.787067Z",
     "shell.execute_reply": "2025-10-28T03:44:48.785744Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.017036,
     "end_time": "2025-10-28T03:44:48.788660",
     "exception": false,
     "start_time": "2025-10-28T03:44:48.771624",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_Model_7(test: pl.DataFrame) -> float:\n",
    "    \n",
    "    global i_M7, opt_preds\n",
    "    \n",
    "    pred = np.float64( opt_preds[i_M7] )\n",
    "    \n",
    "    # print(f\"---> {pred:,.8f} | Iteration {i_M7}\")\n",
    "    \n",
    "    i_M7 = i_M7 + 1\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793861ae",
   "metadata": {
    "papermill": {
     "duration": 0.007619,
     "end_time": "2025-10-28T03:44:48.804983",
     "exception": false,
     "start_time": "2025-10-28T03:44:48.797364",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39383d41",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-10-28T03:44:48.822279Z",
     "iopub.status.busy": "2025-10-28T03:44:48.821961Z",
     "iopub.status.idle": "2025-10-28T03:44:48.836885Z",
     "shell.execute_reply": "2025-10-28T03:44:48.835573Z"
    },
    "papermill": {
     "duration": 0.025825,
     "end_time": "2025-10-28T03:44:48.838582",
     "exception": false,
     "start_time": "2025-10-28T03:44:48.812757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def predict(test: pl.DataFrame) -> float:\n",
    "    \n",
    "    pred_7 = predict_Model_7(test)        # 17.396\n",
    "    pred_6 = predict_Model_6(test)        # 10.237\n",
    "    pred_5 = predict_Model_5(test)        # 10.217\n",
    "    pred_4 = predict_Model_4(test)        # 10.164\n",
    "    pred_1 = predict_Model_1(test)        # 10.147\n",
    "    pred_2 = predict_Model_2(test)        #  8.093\n",
    "    pred_3 = predict_Model_3(test)        #  ?\n",
    "\n",
    "    # LB = 17.396,\n",
    "    pred =\\\n",
    "        pred_7 * 0.9999977 +\\\n",
    "        pred_6 * 0.0000011 +\\\n",
    "        pred_5 * 0.0000005 +\\\n",
    "        pred_4 * 0.0000004 +\\\n",
    "        pred_1 * 0.0000002 +\\\n",
    "        pred_2 * 0.0000001\n",
    "\n",
    "# ------------------------------- hb\n",
    "    \n",
    "    main_weights = [\n",
    "        0.0000001, \n",
    "        0.0000002, \n",
    "        0.0000004,\n",
    "        0.0000005,\n",
    "        0.0000011, \n",
    "        0.9999977, \n",
    "    ]\n",
    "\n",
    "    asc_desc_weights = [0.70, 0.30]\n",
    "    \n",
    "    correct_weights = [ 11, 4, 0, -2, -5, -8 ]\n",
    "    # #     +0.00000010,+0.00000007,+0.00000004,\n",
    "    # #     -0.00000004,-0.00000007,-0.00000010\n",
    "    # # ] \n",
    "    correct_weights = [cw/100_000_000 for cw in correct_weights]\n",
    "\n",
    "       \n",
    "    preds = [\n",
    "        { 'wts':main_weights[0],'pred':pred_1,'res':0 },\n",
    "        { 'wts':main_weights[1],'pred':pred_2,'res':0 },\n",
    "        { 'wts':main_weights[2],'pred':pred_4,'res':0 },\n",
    "        { 'wts':main_weights[3],'pred':pred_5,'res':0 },\n",
    "        { 'wts':main_weights[4],'pred':pred_6,'res':0 },\n",
    "        { 'wts':main_weights[5],'pred':pred_7,'res':0 },\n",
    "    ]\n",
    "    \n",
    "    ascs  = sorted(copy.deepcopy(preds), key=lambda _:_['pred'],reverse=False)\n",
    "    descs = sorted(copy.deepcopy(preds), key=lambda _:_['pred'],reverse=True)\n",
    "    # --------------------------------------------------------------------------\n",
    "    print(\"asc:\",'\\n')\n",
    "    for asc  in ascs:  print(asc)\n",
    "    print('\\n',\"desc:\",'\\n')\n",
    "    for desc in descs: print(desc)\n",
    "    print(\"-\"*21,'\\n')\n",
    "    # ==========================================================================\n",
    "\n",
    "    print('\\n\\n\\t\\t\\t CORRECT.. \\n\\n')\n",
    "    \n",
    "    for asc, c_wts in zip(ascs,  correct_weights): \n",
    "        asc ['res'] = asc ['pred'] * (asc ['wts'] +c_wts)  \n",
    "    for desc,c_wts in zip(descs, correct_weights): \n",
    "        desc['res'] = desc['pred'] * (desc['wts'] +c_wts)\n",
    "    # --------------------------------------------------------------------------\n",
    "    print(\"asc:\",'\\n')\n",
    "    for asc  in ascs:  print(asc)\n",
    "    print('\\n',\"desc:\",'\\n')\n",
    "    for desc in descs: print(desc)\n",
    "    print(\"=\"*21,'\\n')\n",
    "    # ==========================================================================\n",
    "\n",
    "    print('\\n\\n\\t\\t\\t RESULTS ASC/DESC.. \\n\\n')\n",
    "    \n",
    "    result_asc  = sum([asc ['res'] for asc  in ascs ])\n",
    "    result_desc = sum([desc['res'] for desc in descs])\n",
    "    # --------------------------------------------------------------------------\n",
    "    print(f\"asc={result_asc},  desc={result_desc}\", \"\\n\")\n",
    "    # ==========================================================================\n",
    "    \n",
    "    h_blend =\\\n",
    "        result_asc  * asc_desc_weights[0] +\\\n",
    "        result_desc * asc_desc_weights[1]\n",
    "    \n",
    "    print (f\"h-blend={h_blend}\", \"\\n\")\n",
    "    # ==========================================================================\n",
    "\n",
    "    print(f'Model 1 = {pred_1}')\n",
    "    print(f'Model 2 = {pred_2}')\n",
    "    print(f'Model 4 = {pred_4}')\n",
    "    print(f'Model 5 = {pred_5}')\n",
    "    print(f'Model 6 = {pred_6}')\n",
    "    print(f'Model 7 = {pred_7}')\n",
    "    \n",
    "    print(f'==================== previus.pred = {pred}')\n",
    "    print(f'==================== h-blend.pred = {h_blend}\\n')\n",
    "    \n",
    "    if h_blend < 0: return pred  # LB = 17.396  \n",
    "    \n",
    "    return h_blend\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d5e639",
   "metadata": {
    "papermill": {
     "duration": 0.007539,
     "end_time": "2025-10-28T03:44:48.855579",
     "exception": false,
     "start_time": "2025-10-28T03:44:48.848040",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a7004a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T03:44:48.872934Z",
     "iopub.status.busy": "2025-10-28T03:44:48.872599Z",
     "iopub.status.idle": "2025-10-28T03:44:49.624724Z",
     "shell.execute_reply": "2025-10-28T03:44:49.622716Z"
    },
    "papermill": {
     "duration": 0.763386,
     "end_time": "2025-10-28T03:44:49.626535",
     "exception": false,
     "start_time": "2025-10-28T03:44:48.863149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "8980\n",
      "0.0\n",
      "asc: \n",
      "\n",
      "{'wts': 1e-07, 'pred': 0, 'res': 0}\n",
      "{'wts': 2e-07, 'pred': 0.0, 'res': 0}\n",
      "{'wts': 4e-07, 'pred': 0.0, 'res': 0}\n",
      "{'wts': 5e-07, 'pred': 0.0, 'res': 0}\n",
      "{'wts': 1.1e-06, 'pred': 0.0, 'res': 0}\n",
      "{'wts': 0.9999977, 'pred': 0.09849933744168436, 'res': 0}\n",
      "\n",
      " desc: \n",
      "\n",
      "{'wts': 0.9999977, 'pred': 0.09849933744168436, 'res': 0}\n",
      "{'wts': 1e-07, 'pred': 0, 'res': 0}\n",
      "{'wts': 2e-07, 'pred': 0.0, 'res': 0}\n",
      "{'wts': 4e-07, 'pred': 0.0, 'res': 0}\n",
      "{'wts': 5e-07, 'pred': 0.0, 'res': 0}\n",
      "{'wts': 1.1e-06, 'pred': 0.0, 'res': 0}\n",
      "--------------------- \n",
      "\n",
      "\n",
      "\n",
      "\t\t\t CORRECT.. \n",
      "\n",
      "\n",
      "asc: \n",
      "\n",
      "{'wts': 1e-07, 'pred': 0, 'res': 0.0}\n",
      "{'wts': 2e-07, 'pred': 0.0, 'res': 0.0}\n",
      "{'wts': 4e-07, 'pred': 0.0, 'res': 0.0}\n",
      "{'wts': 5e-07, 'pred': 0.0, 'res': 0.0}\n",
      "{'wts': 1.1e-06, 'pred': 0.0, 'res': 0.0}\n",
      "{'wts': 0.9999977, 'pred': 0.09849933744168436, 'res': 0.09849910301326126}\n",
      "\n",
      " desc: \n",
      "\n",
      "{'wts': 0.9999977, 'pred': 0.09849933744168436, 'res': 0.09849912172813537}\n",
      "{'wts': 1e-07, 'pred': 0, 'res': 0.0}\n",
      "{'wts': 2e-07, 'pred': 0.0, 'res': 0.0}\n",
      "{'wts': 4e-07, 'pred': 0.0, 'res': 0.0}\n",
      "{'wts': 5e-07, 'pred': 0.0, 'res': 0.0}\n",
      "{'wts': 1.1e-06, 'pred': 0.0, 'res': 0.0}\n",
      "===================== \n",
      "\n",
      "\n",
      "\n",
      "\t\t\t RESULTS ASC/DESC.. \n",
      "\n",
      "\n",
      "asc=0.09849910301326126,  desc=0.09849912172813537 \n",
      "\n",
      "h-blend=0.09849910862772349 \n",
      "\n",
      "Model 1 = 0\n",
      "Model 2 = 0.0\n",
      "Model 4 = 0.0\n",
      "Model 5 = 0.0\n",
      "Model 6 = 0.0\n",
      "Model 7 = 0.09849933744168436\n",
      "==================== previus.pred = 0.09849911089320824\n",
      "==================== h-blend.pred = 0.09849910862772349\n",
      "\n",
      "0\n",
      "8981\n",
      "0.0\n",
      "asc: \n",
      "\n",
      "{'wts': 1e-07, 'pred': 0, 'res': 0}\n",
      "{'wts': 2e-07, 'pred': 0.0, 'res': 0}\n",
      "{'wts': 4e-07, 'pred': 0.0, 'res': 0}\n",
      "{'wts': 5e-07, 'pred': 0.0, 'res': 0}\n",
      "{'wts': 1.1e-06, 'pred': 0.0, 'res': 0}\n",
      "{'wts': 0.9999977, 'pred': 0.05235790499071848, 'res': 0}\n",
      "\n",
      " desc: \n",
      "\n",
      "{'wts': 0.9999977, 'pred': 0.05235790499071848, 'res': 0}\n",
      "{'wts': 1e-07, 'pred': 0, 'res': 0}\n",
      "{'wts': 2e-07, 'pred': 0.0, 'res': 0}\n",
      "{'wts': 4e-07, 'pred': 0.0, 'res': 0}\n",
      "{'wts': 5e-07, 'pred': 0.0, 'res': 0}\n",
      "{'wts': 1.1e-06, 'pred': 0.0, 'res': 0}\n",
      "--------------------- \n",
      "\n",
      "\n",
      "\n",
      "\t\t\t CORRECT.. \n",
      "\n",
      "\n",
      "asc: \n",
      "\n",
      "{'wts': 1e-07, 'pred': 0, 'res': 0.0}\n",
      "{'wts': 2e-07, 'pred': 0.0, 'res': 0.0}\n",
      "{'wts': 4e-07, 'pred': 0.0, 'res': 0.0}\n",
      "{'wts': 5e-07, 'pred': 0.0, 'res': 0.0}\n",
      "{'wts': 1.1e-06, 'pred': 0.0, 'res': 0.0}\n",
      "{'wts': 0.9999977, 'pred': 0.05235790499071848, 'res': 0.05235778037890461}\n",
      "\n",
      " desc: \n",
      "\n",
      "{'wts': 0.9999977, 'pred': 0.05235790499071848, 'res': 0.05235779032690655}\n",
      "{'wts': 1e-07, 'pred': 0, 'res': 0.0}\n",
      "{'wts': 2e-07, 'pred': 0.0, 'res': 0.0}\n",
      "{'wts': 4e-07, 'pred': 0.0, 'res': 0.0}\n",
      "{'wts': 5e-07, 'pred': 0.0, 'res': 0.0}\n",
      "{'wts': 1.1e-06, 'pred': 0.0, 'res': 0.0}\n",
      "===================== \n",
      "\n",
      "\n",
      "\n",
      "\t\t\t RESULTS ASC/DESC.. \n",
      "\n",
      "\n",
      "asc=0.05235778037890461,  desc=0.05235779032690655 \n",
      "\n",
      "h-blend=0.05235778336330518 \n",
      "\n",
      "Model 1 = 0\n",
      "Model 2 = 0.0\n",
      "Model 4 = 0.0\n",
      "Model 5 = 0.0\n",
      "Model 6 = 0.0\n",
      "Model 7 = 0.05235790499071848\n",
      "==================== previus.pred = 0.052357784567537\n",
      "==================== h-blend.pred = 0.05235778336330518\n",
      "\n",
      "2\n",
      "8982\n",
      "2.0\n",
      "asc: \n",
      "\n",
      "{'wts': 0.9999977, 'pred': 5.40177816976203e-09, 'res': 0}\n",
      "{'wts': 1.1e-06, 'pred': 0.09, 'res': 0}\n",
      "{'wts': 5e-07, 'pred': 0.6001322487531852, 'res': 0}\n",
      "{'wts': 4e-07, 'pred': 0.80007, 'res': 0}\n",
      "{'wts': 1e-07, 'pred': 2, 'res': 0}\n",
      "{'wts': 2e-07, 'pred': 2.0, 'res': 0}\n",
      "\n",
      " desc: \n",
      "\n",
      "{'wts': 1e-07, 'pred': 2, 'res': 0}\n",
      "{'wts': 2e-07, 'pred': 2.0, 'res': 0}\n",
      "{'wts': 4e-07, 'pred': 0.80007, 'res': 0}\n",
      "{'wts': 5e-07, 'pred': 0.6001322487531852, 'res': 0}\n",
      "{'wts': 1.1e-06, 'pred': 0.09, 'res': 0}\n",
      "{'wts': 0.9999977, 'pred': 5.40177816976203e-09, 'res': 0}\n",
      "--------------------- \n",
      "\n",
      "\n",
      "\n",
      "\t\t\t CORRECT.. \n",
      "\n",
      "\n",
      "asc: \n",
      "\n",
      "{'wts': 0.9999977, 'pred': 5.40177816976203e-09, 'res': 5.401766339867838e-09}\n",
      "{'wts': 1.1e-06, 'pred': 0.09, 'res': 1.026e-07}\n",
      "{'wts': 5e-07, 'pred': 0.6001322487531852, 'res': 3.000661243765926e-07}\n",
      "{'wts': 4e-07, 'pred': 0.80007, 'res': 3.0402659999999995e-07}\n",
      "{'wts': 1e-07, 'pred': 2, 'res': 1e-07}\n",
      "{'wts': 2e-07, 'pred': 2.0, 'res': 2.4e-07}\n",
      "\n",
      " desc: \n",
      "\n",
      "{'wts': 1e-07, 'pred': 2, 'res': 4.2e-07}\n",
      "{'wts': 2e-07, 'pred': 2.0, 'res': 4.8e-07}\n",
      "{'wts': 4e-07, 'pred': 0.80007, 'res': 3.20028e-07}\n",
      "{'wts': 5e-07, 'pred': 0.6001322487531852, 'res': 2.8806347940152884e-07}\n",
      "{'wts': 1.1e-06, 'pred': 0.09, 'res': 9.45e-08}\n",
      "{'wts': 0.9999977, 'pred': 5.40177816976203e-09, 'res': 5.401765313529986e-09}\n",
      "===================== \n",
      "\n",
      "\n",
      "\n",
      "\t\t\t RESULTS ASC/DESC.. \n",
      "\n",
      "\n",
      "asc=1.0520944907164603e-06,  desc=1.6079932447150586e-06 \n",
      "\n",
      "h-blend=1.2188641169160396e-06 \n",
      "\n",
      "Model 1 = 2\n",
      "Model 2 = 2.0\n",
      "Model 4 = 0.80007\n",
      "Model 5 = 0.6001322487531852\n",
      "Model 6 = 0.09\n",
      "Model 7 = 5.40177816976203e-09\n",
      "==================== previus.pred = 1.3244958901222647e-06\n",
      "==================== h-blend.pred = 1.2188641169160396e-06\n",
      "\n",
      "2\n",
      "8983\n",
      "2.0\n",
      "asc: \n",
      "\n",
      "{'wts': 0.9999977, 'pred': 5.40177816976203e-09, 'res': 0}\n",
      "{'wts': 1.1e-06, 'pred': 0.09, 'res': 0}\n",
      "{'wts': 5e-07, 'pred': 0.6001322487531852, 'res': 0}\n",
      "{'wts': 4e-07, 'pred': 0.80007, 'res': 0}\n",
      "{'wts': 1e-07, 'pred': 2, 'res': 0}\n",
      "{'wts': 2e-07, 'pred': 2.0, 'res': 0}\n",
      "\n",
      " desc: \n",
      "\n",
      "{'wts': 1e-07, 'pred': 2, 'res': 0}\n",
      "{'wts': 2e-07, 'pred': 2.0, 'res': 0}\n",
      "{'wts': 4e-07, 'pred': 0.80007, 'res': 0}\n",
      "{'wts': 5e-07, 'pred': 0.6001322487531852, 'res': 0}\n",
      "{'wts': 1.1e-06, 'pred': 0.09, 'res': 0}\n",
      "{'wts': 0.9999977, 'pred': 5.40177816976203e-09, 'res': 0}\n",
      "--------------------- \n",
      "\n",
      "\n",
      "\n",
      "\t\t\t CORRECT.. \n",
      "\n",
      "\n",
      "asc: \n",
      "\n",
      "{'wts': 0.9999977, 'pred': 5.40177816976203e-09, 'res': 5.401766339867838e-09}\n",
      "{'wts': 1.1e-06, 'pred': 0.09, 'res': 1.026e-07}\n",
      "{'wts': 5e-07, 'pred': 0.6001322487531852, 'res': 3.000661243765926e-07}\n",
      "{'wts': 4e-07, 'pred': 0.80007, 'res': 3.0402659999999995e-07}\n",
      "{'wts': 1e-07, 'pred': 2, 'res': 1e-07}\n",
      "{'wts': 2e-07, 'pred': 2.0, 'res': 2.4e-07}\n",
      "\n",
      " desc: \n",
      "\n",
      "{'wts': 1e-07, 'pred': 2, 'res': 4.2e-07}\n",
      "{'wts': 2e-07, 'pred': 2.0, 'res': 4.8e-07}\n",
      "{'wts': 4e-07, 'pred': 0.80007, 'res': 3.20028e-07}\n",
      "{'wts': 5e-07, 'pred': 0.6001322487531852, 'res': 2.8806347940152884e-07}\n",
      "{'wts': 1.1e-06, 'pred': 0.09, 'res': 9.45e-08}\n",
      "{'wts': 0.9999977, 'pred': 5.40177816976203e-09, 'res': 5.401765313529986e-09}\n",
      "===================== \n",
      "\n",
      "\n",
      "\n",
      "\t\t\t RESULTS ASC/DESC.. \n",
      "\n",
      "\n",
      "asc=1.0520944907164603e-06,  desc=1.6079932447150586e-06 \n",
      "\n",
      "h-blend=1.2188641169160396e-06 \n",
      "\n",
      "Model 1 = 2\n",
      "Model 2 = 2.0\n",
      "Model 4 = 0.80007\n",
      "Model 5 = 0.6001322487531852\n",
      "Model 6 = 0.09\n",
      "Model 7 = 5.40177816976203e-09\n",
      "==================== previus.pred = 1.3244958901222647e-06\n",
      "==================== h-blend.pred = 1.2188641169160396e-06\n",
      "\n",
      "0\n",
      "8984\n",
      "0.0\n",
      "asc: \n",
      "\n",
      "{'wts': 1e-07, 'pred': 0, 'res': 0}\n",
      "{'wts': 2e-07, 'pred': 0.0, 'res': 0}\n",
      "{'wts': 4e-07, 'pred': 0.0, 'res': 0}\n",
      "{'wts': 5e-07, 'pred': 0.0, 'res': 0}\n",
      "{'wts': 1.1e-06, 'pred': 0.0, 'res': 0}\n",
      "{'wts': 0.9999977, 'pred': 5.40177816976203e-09, 'res': 0}\n",
      "\n",
      " desc: \n",
      "\n",
      "{'wts': 0.9999977, 'pred': 5.40177816976203e-09, 'res': 0}\n",
      "{'wts': 1e-07, 'pred': 0, 'res': 0}\n",
      "{'wts': 2e-07, 'pred': 0.0, 'res': 0}\n",
      "{'wts': 4e-07, 'pred': 0.0, 'res': 0}\n",
      "{'wts': 5e-07, 'pred': 0.0, 'res': 0}\n",
      "{'wts': 1.1e-06, 'pred': 0.0, 'res': 0}\n",
      "--------------------- \n",
      "\n",
      "\n",
      "\n",
      "\t\t\t CORRECT.. \n",
      "\n",
      "\n",
      "asc: \n",
      "\n",
      "{'wts': 1e-07, 'pred': 0, 'res': 0.0}\n",
      "{'wts': 2e-07, 'pred': 0.0, 'res': 0.0}\n",
      "{'wts': 4e-07, 'pred': 0.0, 'res': 0.0}\n",
      "{'wts': 5e-07, 'pred': 0.0, 'res': 0.0}\n",
      "{'wts': 1.1e-06, 'pred': 0.0, 'res': 0.0}\n",
      "{'wts': 0.9999977, 'pred': 5.40177816976203e-09, 'res': 5.401765313529986e-09}\n",
      "\n",
      " desc: \n",
      "\n",
      "{'wts': 0.9999977, 'pred': 5.40177816976203e-09, 'res': 5.401766339867838e-09}\n",
      "{'wts': 1e-07, 'pred': 0, 'res': 0.0}\n",
      "{'wts': 2e-07, 'pred': 0.0, 'res': 0.0}\n",
      "{'wts': 4e-07, 'pred': 0.0, 'res': 0.0}\n",
      "{'wts': 5e-07, 'pred': 0.0, 'res': 0.0}\n",
      "{'wts': 1.1e-06, 'pred': 0.0, 'res': 0.0}\n",
      "===================== \n",
      "\n",
      "\n",
      "\n",
      "\t\t\t RESULTS ASC/DESC.. \n",
      "\n",
      "\n",
      "asc=5.401765313529986e-09,  desc=5.401766339867838e-09 \n",
      "\n",
      "h-blend=5.401765621431342e-09 \n",
      "\n",
      "Model 1 = 0\n",
      "Model 2 = 0.0\n",
      "Model 4 = 0.0\n",
      "Model 5 = 0.0\n",
      "Model 6 = 0.0\n",
      "Model 7 = 5.40177816976203e-09\n",
      "==================== previus.pred = 5.40176574567224e-09\n",
      "==================== h-blend.pred = 5.401765621431342e-09\n",
      "\n",
      "2\n",
      "8985\n",
      "1.7961389013459321\n",
      "asc: \n",
      "\n",
      "{'wts': 0.9999977, 'pred': 5.40177816976203e-09, 'res': 0}\n",
      "{'wts': 1.1e-06, 'pred': 0.09, 'res': 0}\n",
      "{'wts': 5e-07, 'pred': 0.6001322487531852, 'res': 0}\n",
      "{'wts': 4e-07, 'pred': 0.80007, 'res': 0}\n",
      "{'wts': 2e-07, 'pred': 1.7961389013459321, 'res': 0}\n",
      "{'wts': 1e-07, 'pred': 2, 'res': 0}\n",
      "\n",
      " desc: \n",
      "\n",
      "{'wts': 1e-07, 'pred': 2, 'res': 0}\n",
      "{'wts': 2e-07, 'pred': 1.7961389013459321, 'res': 0}\n",
      "{'wts': 4e-07, 'pred': 0.80007, 'res': 0}\n",
      "{'wts': 5e-07, 'pred': 0.6001322487531852, 'res': 0}\n",
      "{'wts': 1.1e-06, 'pred': 0.09, 'res': 0}\n",
      "{'wts': 0.9999977, 'pred': 5.40177816976203e-09, 'res': 0}\n",
      "--------------------- \n",
      "\n",
      "\n",
      "\n",
      "\t\t\t CORRECT.. \n",
      "\n",
      "\n",
      "asc: \n",
      "\n",
      "{'wts': 0.9999977, 'pred': 5.40177816976203e-09, 'res': 5.401766339867838e-09}\n",
      "{'wts': 1.1e-06, 'pred': 0.09, 'res': 1.026e-07}\n",
      "{'wts': 5e-07, 'pred': 0.6001322487531852, 'res': 3.000661243765926e-07}\n",
      "{'wts': 4e-07, 'pred': 0.80007, 'res': 3.0402659999999995e-07}\n",
      "{'wts': 2e-07, 'pred': 1.7961389013459321, 'res': 2.694208352018898e-07}\n",
      "{'wts': 1e-07, 'pred': 2, 'res': 3.999999999999999e-08}\n",
      "\n",
      " desc: \n",
      "\n",
      "{'wts': 1e-07, 'pred': 2, 'res': 4.2e-07}\n",
      "{'wts': 2e-07, 'pred': 1.7961389013459321, 'res': 4.310733363230237e-07}\n",
      "{'wts': 4e-07, 'pred': 0.80007, 'res': 3.20028e-07}\n",
      "{'wts': 5e-07, 'pred': 0.6001322487531852, 'res': 2.8806347940152884e-07}\n",
      "{'wts': 1.1e-06, 'pred': 0.09, 'res': 9.45e-08}\n",
      "{'wts': 0.9999977, 'pred': 5.40177816976203e-09, 'res': 5.401765313529986e-09}\n",
      "===================== \n",
      "\n",
      "\n",
      "\n",
      "\t\t\t RESULTS ASC/DESC.. \n",
      "\n",
      "\n",
      "asc=1.0215153259183502e-06,  desc=1.5590665810380824e-06 \n",
      "\n",
      "h-blend=1.1827807024542697e-06 \n",
      "\n",
      "Model 1 = 2\n",
      "Model 2 = 1.7961389013459321\n",
      "Model 4 = 0.80007\n",
      "Model 5 = 0.6001322487531852\n",
      "Model 6 = 0.09\n",
      "Model 7 = 5.40177816976203e-09\n",
      "==================== previus.pred = 1.304109780256858e-06\n",
      "==================== h-blend.pred = 1.1827807024542697e-06\n",
      "\n",
      "2\n",
      "8986\n",
      "1.738006723104844\n",
      "asc: \n",
      "\n",
      "{'wts': 0.9999977, 'pred': 5.40177816976203e-09, 'res': 0}\n",
      "{'wts': 1.1e-06, 'pred': 0.09, 'res': 0}\n",
      "{'wts': 5e-07, 'pred': 0.6001322487531852, 'res': 0}\n",
      "{'wts': 4e-07, 'pred': 0.80007, 'res': 0}\n",
      "{'wts': 2e-07, 'pred': 1.738006723104844, 'res': 0}\n",
      "{'wts': 1e-07, 'pred': 2, 'res': 0}\n",
      "\n",
      " desc: \n",
      "\n",
      "{'wts': 1e-07, 'pred': 2, 'res': 0}\n",
      "{'wts': 2e-07, 'pred': 1.738006723104844, 'res': 0}\n",
      "{'wts': 4e-07, 'pred': 0.80007, 'res': 0}\n",
      "{'wts': 5e-07, 'pred': 0.6001322487531852, 'res': 0}\n",
      "{'wts': 1.1e-06, 'pred': 0.09, 'res': 0}\n",
      "{'wts': 0.9999977, 'pred': 5.40177816976203e-09, 'res': 0}\n",
      "--------------------- \n",
      "\n",
      "\n",
      "\n",
      "\t\t\t CORRECT.. \n",
      "\n",
      "\n",
      "asc: \n",
      "\n",
      "{'wts': 0.9999977, 'pred': 5.40177816976203e-09, 'res': 5.401766339867838e-09}\n",
      "{'wts': 1.1e-06, 'pred': 0.09, 'res': 1.026e-07}\n",
      "{'wts': 5e-07, 'pred': 0.6001322487531852, 'res': 3.000661243765926e-07}\n",
      "{'wts': 4e-07, 'pred': 0.80007, 'res': 3.0402659999999995e-07}\n",
      "{'wts': 2e-07, 'pred': 1.738006723104844, 'res': 2.6070100846572656e-07}\n",
      "{'wts': 1e-07, 'pred': 2, 'res': 3.999999999999999e-08}\n",
      "\n",
      " desc: \n",
      "\n",
      "{'wts': 1e-07, 'pred': 2, 'res': 4.2e-07}\n",
      "{'wts': 2e-07, 'pred': 1.738006723104844, 'res': 4.171216135451625e-07}\n",
      "{'wts': 4e-07, 'pred': 0.80007, 'res': 3.20028e-07}\n",
      "{'wts': 5e-07, 'pred': 0.6001322487531852, 'res': 2.8806347940152884e-07}\n",
      "{'wts': 1.1e-06, 'pred': 0.09, 'res': 9.45e-08}\n",
      "{'wts': 0.9999977, 'pred': 5.40177816976203e-09, 'res': 5.401765313529986e-09}\n",
      "===================== \n",
      "\n",
      "\n",
      "\n",
      "\t\t\t RESULTS ASC/DESC.. \n",
      "\n",
      "\n",
      "asc=1.012795499182187e-06,  desc=1.5451148582602214e-06 \n",
      "\n",
      "h-blend=1.1724913069055972e-06 \n",
      "\n",
      "Model 1 = 2\n",
      "Model 2 = 1.738006723104844\n",
      "Model 4 = 0.80007\n",
      "Model 5 = 0.6001322487531852\n",
      "Model 6 = 0.09\n",
      "Model 7 = 5.40177816976203e-09\n",
      "==================== previus.pred = 1.298296562432749e-06\n",
      "==================== h-blend.pred = 1.1724913069055972e-06\n",
      "\n",
      "2\n",
      "8987\n",
      "1.969601279427732\n",
      "asc: \n",
      "\n",
      "{'wts': 0.9999977, 'pred': 0.046486640131093174, 'res': 0}\n",
      "{'wts': 1.1e-06, 'pred': 0.09, 'res': 0}\n",
      "{'wts': 5e-07, 'pred': 0.6001322487531852, 'res': 0}\n",
      "{'wts': 4e-07, 'pred': 0.80007, 'res': 0}\n",
      "{'wts': 2e-07, 'pred': 1.969601279427732, 'res': 0}\n",
      "{'wts': 1e-07, 'pred': 2, 'res': 0}\n",
      "\n",
      " desc: \n",
      "\n",
      "{'wts': 1e-07, 'pred': 2, 'res': 0}\n",
      "{'wts': 2e-07, 'pred': 1.969601279427732, 'res': 0}\n",
      "{'wts': 4e-07, 'pred': 0.80007, 'res': 0}\n",
      "{'wts': 5e-07, 'pred': 0.6001322487531852, 'res': 0}\n",
      "{'wts': 1.1e-06, 'pred': 0.09, 'res': 0}\n",
      "{'wts': 0.9999977, 'pred': 0.046486640131093174, 'res': 0}\n",
      "--------------------- \n",
      "\n",
      "\n",
      "\n",
      "\t\t\t CORRECT.. \n",
      "\n",
      "\n",
      "asc: \n",
      "\n",
      "{'wts': 0.9999977, 'pred': 0.046486640131093174, 'res': 0.04648653832535129}\n",
      "{'wts': 1.1e-06, 'pred': 0.09, 'res': 1.026e-07}\n",
      "{'wts': 5e-07, 'pred': 0.6001322487531852, 'res': 3.000661243765926e-07}\n",
      "{'wts': 4e-07, 'pred': 0.80007, 'res': 3.0402659999999995e-07}\n",
      "{'wts': 2e-07, 'pred': 1.969601279427732, 'res': 2.954401919141598e-07}\n",
      "{'wts': 1e-07, 'pred': 2, 'res': 3.999999999999999e-08}\n",
      "\n",
      " desc: \n",
      "\n",
      "{'wts': 1e-07, 'pred': 2, 'res': 4.2e-07}\n",
      "{'wts': 2e-07, 'pred': 1.969601279427732, 'res': 4.7270430706265566e-07}\n",
      "{'wts': 4e-07, 'pred': 0.80007, 'res': 3.20028e-07}\n",
      "{'wts': 5e-07, 'pred': 0.6001322487531852, 'res': 2.8806347940152884e-07}\n",
      "{'wts': 1.1e-06, 'pred': 0.09, 'res': 9.45e-08}\n",
      "{'wts': 0.9999977, 'pred': 0.046486640131093174, 'res': 0.046486529492889665}\n",
      "===================== \n",
      "\n",
      "\n",
      "\n",
      "\t\t\t RESULTS ASC/DESC.. \n",
      "\n",
      "\n",
      "asc=0.046487580458267584,  desc=0.04648812478867613 \n",
      "\n",
      "h-blend=0.04648774375739014 \n",
      "\n",
      "Model 1 = 2\n",
      "Model 2 = 1.969601279427732\n",
      "Model 4 = 0.80007\n",
      "Model 5 = 0.6001322487531852\n",
      "Model 6 = 0.09\n",
      "Model 7 = 0.046486640131093174\n",
      "==================== previus.pred = 0.046487849266073196\n",
      "==================== h-blend.pred = 0.04648774375739014\n",
      "\n",
      "2\n",
      "8988\n",
      "2.0\n",
      "asc: \n",
      "\n",
      "{'wts': 1.1e-06, 'pred': 0.09, 'res': 0}\n",
      "{'wts': 0.9999977, 'pred': 0.10261887443131651, 'res': 0}\n",
      "{'wts': 5e-07, 'pred': 0.6001322487531852, 'res': 0}\n",
      "{'wts': 4e-07, 'pred': 0.80007, 'res': 0}\n",
      "{'wts': 1e-07, 'pred': 2, 'res': 0}\n",
      "{'wts': 2e-07, 'pred': 2.0, 'res': 0}\n",
      "\n",
      " desc: \n",
      "\n",
      "{'wts': 1e-07, 'pred': 2, 'res': 0}\n",
      "{'wts': 2e-07, 'pred': 2.0, 'res': 0}\n",
      "{'wts': 4e-07, 'pred': 0.80007, 'res': 0}\n",
      "{'wts': 5e-07, 'pred': 0.6001322487531852, 'res': 0}\n",
      "{'wts': 0.9999977, 'pred': 0.10261887443131651, 'res': 0}\n",
      "{'wts': 1.1e-06, 'pred': 0.09, 'res': 0}\n",
      "--------------------- \n",
      "\n",
      "\n",
      "\n",
      "\t\t\t CORRECT.. \n",
      "\n",
      "\n",
      "asc: \n",
      "\n",
      "{'wts': 1.1e-06, 'pred': 0.09, 'res': 1.089e-07}\n",
      "{'wts': 0.9999977, 'pred': 0.10261887443131651, 'res': 0.10261864251266029}\n",
      "{'wts': 5e-07, 'pred': 0.6001322487531852, 'res': 3.000661243765926e-07}\n",
      "{'wts': 4e-07, 'pred': 0.80007, 'res': 3.0402659999999995e-07}\n",
      "{'wts': 1e-07, 'pred': 2, 'res': 1e-07}\n",
      "{'wts': 2e-07, 'pred': 2.0, 'res': 2.4e-07}\n",
      "\n",
      " desc: \n",
      "\n",
      "{'wts': 1e-07, 'pred': 2, 'res': 4.2e-07}\n",
      "{'wts': 2e-07, 'pred': 2.0, 'res': 4.8e-07}\n",
      "{'wts': 4e-07, 'pred': 0.80007, 'res': 3.20028e-07}\n",
      "{'wts': 5e-07, 'pred': 0.6001322487531852, 'res': 2.8806347940152884e-07}\n",
      "{'wts': 0.9999977, 'pred': 0.10261887443131651, 'res': 0.1026186332769616}\n",
      "{'wts': 1.1e-06, 'pred': 0.09, 'res': 9.179999999999999e-08}\n",
      "===================== \n",
      "\n",
      "\n",
      "\n",
      "\t\t\t RESULTS ASC/DESC.. \n",
      "\n",
      "\n",
      "asc=0.10261969550538468,  desc=0.102620233168441 \n",
      "\n",
      "h-blend=0.10261985680430158 \n",
      "\n",
      "Model 1 = 2\n",
      "Model 2 = 2.0\n",
      "Model 4 = 0.80007\n",
      "Model 5 = 0.6001322487531852\n",
      "Model 6 = 0.09\n",
      "Model 7 = 0.10261887443131651\n",
      "==================== previus.pred = 0.10261995750202971\n",
      "==================== h-blend.pred = 0.10261985680430158\n",
      "\n",
      "2\n",
      "8989\n",
      "0.852732920685728\n",
      "asc: \n",
      "\n",
      "{'wts': 0.9999977, 'pred': 5.40177816976203e-09, 'res': 0}\n",
      "{'wts': 1.1e-06, 'pred': 0.09, 'res': 0}\n",
      "{'wts': 5e-07, 'pred': 0.6001322487531852, 'res': 0}\n",
      "{'wts': 4e-07, 'pred': 0.80007, 'res': 0}\n",
      "{'wts': 2e-07, 'pred': 0.852732920685728, 'res': 0}\n",
      "{'wts': 1e-07, 'pred': 2, 'res': 0}\n",
      "\n",
      " desc: \n",
      "\n",
      "{'wts': 1e-07, 'pred': 2, 'res': 0}\n",
      "{'wts': 2e-07, 'pred': 0.852732920685728, 'res': 0}\n",
      "{'wts': 4e-07, 'pred': 0.80007, 'res': 0}\n",
      "{'wts': 5e-07, 'pred': 0.6001322487531852, 'res': 0}\n",
      "{'wts': 1.1e-06, 'pred': 0.09, 'res': 0}\n",
      "{'wts': 0.9999977, 'pred': 5.40177816976203e-09, 'res': 0}\n",
      "--------------------- \n",
      "\n",
      "\n",
      "\n",
      "\t\t\t CORRECT.. \n",
      "\n",
      "\n",
      "asc: \n",
      "\n",
      "{'wts': 0.9999977, 'pred': 5.40177816976203e-09, 'res': 5.401766339867838e-09}\n",
      "{'wts': 1.1e-06, 'pred': 0.09, 'res': 1.026e-07}\n",
      "{'wts': 5e-07, 'pred': 0.6001322487531852, 'res': 3.000661243765926e-07}\n",
      "{'wts': 4e-07, 'pred': 0.80007, 'res': 3.0402659999999995e-07}\n",
      "{'wts': 2e-07, 'pred': 0.852732920685728, 'res': 1.279099381028592e-07}\n",
      "{'wts': 1e-07, 'pred': 2, 'res': 3.999999999999999e-08}\n",
      "\n",
      " desc: \n",
      "\n",
      "{'wts': 1e-07, 'pred': 2, 'res': 4.2e-07}\n",
      "{'wts': 2e-07, 'pred': 0.852732920685728, 'res': 2.0465590096457471e-07}\n",
      "{'wts': 4e-07, 'pred': 0.80007, 'res': 3.20028e-07}\n",
      "{'wts': 5e-07, 'pred': 0.6001322487531852, 'res': 2.8806347940152884e-07}\n",
      "{'wts': 1.1e-06, 'pred': 0.09, 'res': 9.45e-08}\n",
      "{'wts': 0.9999977, 'pred': 5.40177816976203e-09, 'res': 5.401765313529986e-09}\n",
      "===================== \n",
      "\n",
      "\n",
      "\n",
      "\t\t\t RESULTS ASC/DESC.. \n",
      "\n",
      "\n",
      "asc=8.800044288193195e-07,  desc=1.3326491456796334e-06 \n",
      "\n",
      "h-blend=1.0157978438774137e-06 \n",
      "\n",
      "Model 1 = 2\n",
      "Model 2 = 0.852732920685728\n",
      "Model 4 = 0.80007\n",
      "Model 5 = 0.6001322487531852\n",
      "Model 6 = 0.09\n",
      "Model 7 = 5.40177816976203e-09\n",
      "==================== previus.pred = 1.2097691821908374e-06\n",
      "==================== h-blend.pred = 1.0157978438774137e-06\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(('/kaggle/input/hull-tactical-market-prediction/',))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 13750964,
     "sourceId": 111543,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 342.700331,
   "end_time": "2025-10-28T03:44:50.861935",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-28T03:39:08.161604",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
